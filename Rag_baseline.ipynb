{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T14:57:40.259846Z",
     "start_time": "2025-06-03T14:56:47.537531Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\r\n",
      "  Downloading langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\r\n",
      "Collecting langchain-openai\r\n",
      "  Downloading langchain_openai-0.3.19-py3-none-any.whl.metadata (2.3 kB)\r\n",
      "Collecting sentence-transformers\r\n",
      "  Downloading sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\r\n",
      "Collecting faiss-cpu\r\n",
      "  Downloading faiss_cpu-1.11.0-cp311-cp311-macosx_14_0_arm64.whl.metadata (4.8 kB)\r\n",
      "Collecting tiktoken\r\n",
      "  Downloading tiktoken-0.9.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\r\n",
      "Collecting langchain-core<1.0.0,>=0.3.58 (from langchain)\r\n",
      "  Downloading langchain_core-0.3.63-py3-none-any.whl.metadata (5.8 kB)\r\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\r\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\r\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain)\r\n",
      "  Downloading langsmith-0.3.44-py3-none-any.whl.metadata (15 kB)\r\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\r\n",
      "  Downloading pydantic-2.11.5-py3-none-any.whl.metadata (67 kB)\r\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\r\n",
      "  Downloading sqlalchemy-2.0.41-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.6 kB)\r\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.11/site-packages (from langchain) (2.32.3)\r\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.11/site-packages (from langchain) (6.0.2)\r\n",
      "Collecting openai<2.0.0,>=1.68.2 (from langchain-openai)\r\n",
      "  Downloading openai-1.83.0-py3-none-any.whl.metadata (25 kB)\r\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\r\n",
      "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\r\n",
      "Collecting tqdm (from sentence-transformers)\r\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\r\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\r\n",
      "  Downloading torch-2.7.0-cp311-none-macosx_11_0_arm64.whl.metadata (29 kB)\r\n",
      "Collecting scikit-learn (from sentence-transformers)\r\n",
      "  Downloading scikit_learn-1.6.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (31 kB)\r\n",
      "Collecting scipy (from sentence-transformers)\r\n",
      "  Downloading scipy-1.15.3-cp311-cp311-macosx_14_0_arm64.whl.metadata (61 kB)\r\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\r\n",
      "  Downloading huggingface_hub-0.32.4-py3-none-any.whl.metadata (14 kB)\r\n",
      "Collecting Pillow (from sentence-transformers)\r\n",
      "  Downloading pillow-11.2.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.9 kB)\r\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in ./.venv/lib/python3.11/site-packages (from sentence-transformers) (4.14.0)\r\n",
      "Collecting numpy<3.0,>=1.25.0 (from faiss-cpu)\r\n",
      "  Downloading numpy-2.2.6-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\r\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.11/site-packages (from faiss-cpu) (25.0)\r\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\r\n",
      "  Downloading regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\r\n",
      "Collecting filelock (from huggingface-hub>=0.20.0->sentence-transformers)\r\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\r\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers)\r\n",
      "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\r\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub>=0.20.0->sentence-transformers)\r\n",
      "  Downloading hf_xet-1.1.2-cp37-abi3-macosx_11_0_arm64.whl.metadata (879 bytes)\r\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<1.0.0,>=0.3.58->langchain)\r\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\r\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.58->langchain)\r\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\r\n",
      "Collecting packaging (from faiss-cpu)\r\n",
      "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\r\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.17->langchain)\r\n",
      "  Downloading orjson-3.10.18-cp311-cp311-macosx_15_0_arm64.whl.metadata (41 kB)\r\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.17->langchain)\r\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\r\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\r\n",
      "  Downloading zstandard-0.23.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.0 kB)\r\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.9.0)\r\n",
      "Collecting distro<2,>=1.7.0 (from openai<2.0.0,>=1.68.2->langchain-openai)\r\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\r\n",
      "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.68.2->langchain-openai)\r\n",
      "  Downloading jiter-0.10.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.2 kB)\r\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.1)\r\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\r\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\r\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3.0.0,>=2.7.4->langchain)\r\n",
      "  Downloading pydantic_core-2.33.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.8 kB)\r\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.7.4->langchain)\r\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2025.4.26)\r\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence-transformers)\r\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\r\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers)\r\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\r\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\r\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\r\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\r\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\r\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\r\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\r\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\r\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\r\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\r\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers)\r\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\r\n",
      "Downloading langchain-0.3.25-py3-none-any.whl (1.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading langchain_openai-0.3.19-py3-none-any.whl (64 kB)\r\n",
      "Downloading sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\r\n",
      "Downloading faiss_cpu-1.11.0-cp311-cp311-macosx_14_0_arm64.whl (3.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\r\n",
      "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-macosx_11_0_arm64.whl (1.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading huggingface_hub-0.32.4-py3-none-any.whl (512 kB)\r\n",
      "Downloading langchain_core-0.3.63-py3-none-any.whl (438 kB)\r\n",
      "Downloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\r\n",
      "Downloading langsmith-0.3.44-py3-none-any.whl (361 kB)\r\n",
      "Downloading numpy-2.2.6-cp311-cp311-macosx_14_0_arm64.whl (5.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\r\n",
      "\u001b[?25hDownloading openai-1.83.0-py3-none-any.whl (723 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m723.4/723.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\r\n",
      "Downloading pydantic-2.11.5-py3-none-any.whl (444 kB)\r\n",
      "Downloading pydantic_core-2.33.2-cp311-cp311-macosx_11_0_arm64.whl (1.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\r\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl (284 kB)\r\n",
      "Downloading sqlalchemy-2.0.41-cp311-cp311-macosx_11_0_arm64.whl (2.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\r\n",
      "\u001b[?25hDownloading torch-2.7.0-cp311-none-macosx_11_0_arm64.whl (68.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\r\n",
      "Downloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\r\n",
      "\u001b[?25hDownloading pillow-11.2.1-cp311-cp311-macosx_11_0_arm64.whl (3.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\r\n",
      "\u001b[?25hDownloading scikit_learn-1.6.1-cp311-cp311-macosx_12_0_arm64.whl (11.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\r\n",
      "\u001b[?25hDownloading scipy-1.15.3-cp311-cp311-macosx_14_0_arm64.whl (22.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.4/22.4 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\r\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\r\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\r\n",
      "Downloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\r\n",
      "Downloading hf_xet-1.1.2-cp37-abi3-macosx_11_0_arm64.whl (2.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\r\n",
      "\u001b[?25hDownloading jiter-0.10.0-cp311-cp311-macosx_11_0_arm64.whl (321 kB)\r\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\r\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\r\n",
      "Downloading orjson-3.10.18-cp311-cp311-macosx_15_0_arm64.whl (133 kB)\r\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\r\n",
      "Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\r\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\r\n",
      "\u001b[?25hDownloading tenacity-9.1.2-py3-none-any.whl (28 kB)\r\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\r\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\r\n",
      "\u001b[?25hDownloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\r\n",
      "Downloading zstandard-0.23.0-cp311-cp311-macosx_11_0_arm64.whl (633 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m633.7/633.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\r\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\r\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: mpmath, zstandard, typing-inspection, tqdm, threadpoolctl, tenacity, sympy, SQLAlchemy, safetensors, regex, pydantic-core, Pillow, packaging, orjson, numpy, networkx, jsonpatch, joblib, jiter, hf-xet, fsspec, filelock, distro, annotated-types, torch, tiktoken, scipy, requests-toolbelt, pydantic, huggingface-hub, faiss-cpu, tokenizers, scikit-learn, openai, langsmith, transformers, langchain-core, sentence-transformers, langchain-text-splitters, langchain-openai, langchain\r\n",
      "  Attempting uninstall: packaging\r\n",
      "    Found existing installation: packaging 25.0\r\n",
      "    Uninstalling packaging-25.0:\r\n",
      "      Successfully uninstalled packaging-25.0\r\n",
      "Successfully installed Pillow-11.2.1 SQLAlchemy-2.0.41 annotated-types-0.7.0 distro-1.9.0 faiss-cpu-1.11.0 filelock-3.18.0 fsspec-2025.5.1 hf-xet-1.1.2 huggingface-hub-0.32.4 jiter-0.10.0 joblib-1.5.1 jsonpatch-1.33 langchain-0.3.25 langchain-core-0.3.63 langchain-openai-0.3.19 langchain-text-splitters-0.3.8 langsmith-0.3.44 mpmath-1.3.0 networkx-3.5 numpy-2.2.6 openai-1.83.0 orjson-3.10.18 packaging-24.2 pydantic-2.11.5 pydantic-core-2.33.2 regex-2024.11.6 requests-toolbelt-1.0.0 safetensors-0.5.3 scikit-learn-1.6.1 scipy-1.15.3 sentence-transformers-4.1.0 sympy-1.14.0 tenacity-9.1.2 threadpoolctl-3.6.0 tiktoken-0.9.0 tokenizers-0.21.1 torch-2.7.0 tqdm-4.67.1 transformers-4.52.4 typing-inspection-0.4.1 zstandard-0.23.0\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain langchain-openai sentence-transformers faiss-cpu tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61275f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-neo4j in ./.venv/lib/python3.11/site-packages (0.4.0)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: langchain-text-splitters in ./.venv/lib/python3.11/site-packages (0.3.8)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement langchain-embeddings (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for langchain-embeddings\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-neo4j langchain-community langchain-text-splitters langchain-embeddings langchain-vectorstores litellm kg-gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a612cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unstructured[local-inference]\n",
      "  Downloading unstructured-0.17.2-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting chardet (from unstructured[local-inference])\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting filetype (from unstructured[local-inference])\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting python-magic (from unstructured[local-inference])\n",
      "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting lxml (from unstructured[local-inference])\n",
      "  Downloading lxml-5.4.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: nltk in ./.venv/lib/python3.11/site-packages (from unstructured[local-inference]) (3.9.1)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from unstructured[local-inference]) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in ./.venv/lib/python3.11/site-packages (from unstructured[local-inference]) (4.13.4)\n",
      "Collecting emoji (from unstructured[local-inference])\n",
      "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: dataclasses-json in ./.venv/lib/python3.11/site-packages (from unstructured[local-inference]) (0.6.7)\n",
      "Collecting python-iso639 (from unstructured[local-inference])\n",
      "  Downloading python_iso639-2025.2.18-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting langdetect (from unstructured[local-inference])\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (from unstructured[local-inference]) (2.2.6)\n",
      "Collecting rapidfuzz (from unstructured[local-inference])\n",
      "  Downloading rapidfuzz-3.13.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: backoff in ./.venv/lib/python3.11/site-packages (from unstructured[local-inference]) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in ./.venv/lib/python3.11/site-packages (from unstructured[local-inference]) (4.14.0)\n",
      "Collecting unstructured-client (from unstructured[local-inference])\n",
      "  Downloading unstructured_client-0.36.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting wrapt (from unstructured[local-inference])\n",
      "  Downloading wrapt-1.17.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.11/site-packages (from unstructured[local-inference]) (4.67.1)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.11/site-packages (from unstructured[local-inference]) (7.0.0)\n",
      "Collecting python-oxmsg (from unstructured[local-inference])\n",
      "  Downloading python_oxmsg-0.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting html5lib (from unstructured[local-inference])\n",
      "  Downloading html5lib-1.1-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Collecting onnxruntime>=1.19.0 (from unstructured[local-inference])\n",
      "  Downloading onnxruntime-1.22.0-cp311-cp311-macosx_13_0_universal2.whl.metadata (4.5 kB)\n",
      "Collecting python-pptx>=1.0.1 (from unstructured[local-inference])\n",
      "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting effdet (from unstructured[local-inference])\n",
      "  Downloading effdet-0.4.1-py3-none-any.whl.metadata (33 kB)\n",
      "Collecting pypandoc (from unstructured[local-inference])\n",
      "  Downloading pypandoc-1.15-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting pikepdf (from unstructured[local-inference])\n",
      "  Downloading pikepdf-9.8.1-cp311-cp311-macosx_14_0_arm64.whl.metadata (8.2 kB)\n",
      "Collecting google-cloud-vision (from unstructured[local-inference])\n",
      "  Downloading google_cloud_vision-3.10.2-py3-none-any.whl.metadata (9.6 kB)\n",
      "Collecting unstructured.pytesseract>=0.3.12 (from unstructured[local-inference])\n",
      "  Downloading unstructured.pytesseract-0.3.15-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pdfminer.six (from unstructured[local-inference])\n",
      "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting unstructured-inference>=0.8.10 (from unstructured[local-inference])\n",
      "  Downloading unstructured_inference-1.0.5-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting xlrd (from unstructured[local-inference])\n",
      "  Downloading xlrd-2.0.2-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from unstructured[local-inference]) (3.5)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (from unstructured[local-inference]) (2.3.0)\n",
      "Collecting pdf2image (from unstructured[local-inference])\n",
      "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting openpyxl (from unstructured[local-inference])\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting python-docx>=1.1.2 (from unstructured[local-inference])\n",
      "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: pypdf in ./.venv/lib/python3.11/site-packages (from unstructured[local-inference]) (5.6.0)\n",
      "Collecting onnx>=1.17.0 (from unstructured[local-inference])\n",
      "  Downloading onnx-1.18.0-cp311-cp311-macosx_12_0_universal2.whl.metadata (6.9 kB)\n",
      "Collecting markdown (from unstructured[local-inference])\n",
      "  Downloading markdown-3.8-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting pi-heif (from unstructured[local-inference])\n",
      "  Downloading pi_heif-0.22.0-cp311-cp311-macosx_14_0_arm64.whl.metadata (6.5 kB)\n",
      "Collecting protobuf>=4.25.1 (from onnx>=1.17.0->unstructured[local-inference])\n",
      "  Downloading protobuf-6.31.1-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Collecting coloredlogs (from onnxruntime>=1.19.0->unstructured[local-inference])\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.19.0->unstructured[local-inference])\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.11/site-packages (from onnxruntime>=1.19.0->unstructured[local-inference]) (24.2)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.11/site-packages (from onnxruntime>=1.19.0->unstructured[local-inference]) (1.14.0)\n",
      "Requirement already satisfied: Pillow>=3.3.2 in ./.venv/lib/python3.11/site-packages (from python-pptx>=1.0.1->unstructured[local-inference]) (11.2.1)\n",
      "Collecting XlsxWriter>=0.5.7 (from python-pptx>=1.0.1->unstructured[local-inference])\n",
      "  Downloading XlsxWriter-3.2.3-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting python-multipart (from unstructured-inference>=0.8.10->unstructured[local-inference])\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: huggingface-hub in ./.venv/lib/python3.11/site-packages (from unstructured-inference>=0.8.10->unstructured[local-inference]) (0.32.4)\n",
      "Collecting opencv-python!=4.7.0.68 (from unstructured-inference>=0.8.10->unstructured[local-inference])\n",
      "  Downloading opencv_python-4.11.0.86-cp37-abi3-macosx_13_0_arm64.whl.metadata (20 kB)\n",
      "Collecting matplotlib (from unstructured-inference>=0.8.10->unstructured[local-inference])\n",
      "  Downloading matplotlib-3.10.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.11/site-packages (from unstructured-inference>=0.8.10->unstructured[local-inference]) (2.7.0)\n",
      "Collecting timm (from unstructured-inference>=0.8.10->unstructured[local-inference])\n",
      "  Downloading timm-1.0.15-py3-none-any.whl.metadata (52 kB)\n",
      "Requirement already satisfied: transformers>=4.25.1 in ./.venv/lib/python3.11/site-packages (from unstructured-inference>=0.8.10->unstructured[local-inference]) (4.52.4)\n",
      "Collecting accelerate (from unstructured-inference>=0.8.10->unstructured[local-inference])\n",
      "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.11/site-packages (from unstructured-inference>=0.8.10->unstructured[local-inference]) (1.15.3)\n",
      "Collecting pypdfium2 (from unstructured-inference>=0.8.10->unstructured[local-inference])\n",
      "  Downloading pypdfium2-4.30.1-py3-none-macosx_11_0_arm64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from transformers>=4.25.1->unstructured-inference>=0.8.10->unstructured[local-inference]) (3.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from transformers>=4.25.1->unstructured-inference>=0.8.10->unstructured[local-inference]) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.11/site-packages (from transformers>=4.25.1->unstructured-inference>=0.8.10->unstructured[local-inference]) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.11/site-packages (from transformers>=4.25.1->unstructured-inference>=0.8.10->unstructured[local-inference]) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.11/site-packages (from transformers>=4.25.1->unstructured-inference>=0.8.10->unstructured[local-inference]) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface-hub->unstructured-inference>=0.8.10->unstructured[local-inference]) (2024.12.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./.venv/lib/python3.11/site-packages (from huggingface-hub->unstructured-inference>=0.8.10->unstructured[local-inference]) (1.1.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch->unstructured-inference>=0.8.10->unstructured[local-inference]) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy->onnxruntime>=1.19.0->unstructured[local-inference]) (1.3.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./.venv/lib/python3.11/site-packages (from beautifulsoup4->unstructured[local-inference]) (2.7)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.19.0->unstructured[local-inference])\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.11/site-packages (from dataclasses-json->unstructured[local-inference]) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.venv/lib/python3.11/site-packages (from dataclasses-json->unstructured[local-inference]) (0.9.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured[local-inference]) (1.1.0)\n",
      "Collecting torchvision (from effdet->unstructured[local-inference])\n",
      "  Downloading torchvision-0.22.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Collecting pycocotools>=2.0.2 (from effdet->unstructured[local-inference])\n",
      "  Downloading pycocotools-2.0.10-cp311-cp311-macosx_10_9_universal2.whl.metadata (1.3 kB)\n",
      "Collecting omegaconf>=2.0 (from effdet->unstructured[local-inference])\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.0->effdet->unstructured[local-inference])\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[local-inference])\n",
      "  Downloading google_api_core-2.25.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 (from google-cloud-vision->unstructured[local-inference])\n",
      "  Downloading google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting proto-plus<2.0.0,>=1.22.3 (from google-cloud-vision->unstructured[local-inference])\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[local-inference])\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting grpcio<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[local-inference])\n",
      "  Downloading grpcio-1.73.0-cp311-cp311-macosx_11_0_universal2.whl.metadata (3.8 kB)\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[local-inference])\n",
      "  Downloading grpcio_status-1.73.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[local-inference])\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[local-inference])\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[local-inference])\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->unstructured[local-inference]) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests->unstructured[local-inference]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->unstructured[local-inference]) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests->unstructured[local-inference]) (2025.4.26)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[local-inference])\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: six>=1.9 in ./.venv/lib/python3.11/site-packages (from html5lib->unstructured[local-inference]) (1.17.0)\n",
      "Requirement already satisfied: webencodings in ./.venv/lib/python3.11/site-packages (from html5lib->unstructured[local-inference]) (0.5.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch->unstructured-inference>=0.8.10->unstructured[local-inference]) (3.0.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->unstructured-inference>=0.8.10->unstructured[local-inference])\n",
      "  Downloading contourpy-1.3.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->unstructured-inference>=0.8.10->unstructured[local-inference])\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->unstructured-inference>=0.8.10->unstructured[local-inference])\n",
      "  Downloading fonttools-4.58.4-cp311-cp311-macosx_10_9_universal2.whl.metadata (106 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->unstructured-inference>=0.8.10->unstructured[local-inference])\n",
      "  Downloading kiwisolver-1.4.8-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->unstructured-inference>=0.8.10->unstructured[local-inference])\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.11/site-packages (from matplotlib->unstructured-inference>=0.8.10->unstructured[local-inference]) (2.9.0.post0)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.11/site-packages (from nltk->unstructured[local-inference]) (8.2.1)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.11/site-packages (from nltk->unstructured[local-inference]) (1.5.1)\n",
      "Collecting et-xmlfile (from openpyxl->unstructured[local-inference])\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas->unstructured[local-inference]) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas->unstructured[local-inference]) (2025.2)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six->unstructured[local-inference])\n",
      "  Downloading cryptography-45.0.4-cp311-abi3-macosx_10_9_universal2.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: cffi>=1.14 in ./.venv/lib/python3.11/site-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[local-inference]) (1.17.1)\n",
      "Requirement already satisfied: pycparser in ./.venv/lib/python3.11/site-packages (from cffi>=1.14->cryptography>=36.0.0->pdfminer.six->unstructured[local-inference]) (2.22)\n",
      "Collecting Deprecated (from pikepdf->unstructured[local-inference])\n",
      "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting olefile (from python-oxmsg->unstructured[local-inference])\n",
      "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting torch (from unstructured-inference>=0.8.10->unstructured[local-inference])\n",
      "  Downloading torch-2.7.1-cp311-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
      "Collecting aiofiles>=24.1.0 (from unstructured-client->unstructured[local-inference])\n",
      "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: httpx>=0.27.0 in ./.venv/lib/python3.11/site-packages (from unstructured-client->unstructured[local-inference]) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.6.0 in ./.venv/lib/python3.11/site-packages (from unstructured-client->unstructured[local-inference]) (1.6.0)\n",
      "Requirement already satisfied: pydantic>=2.11.2 in ./.venv/lib/python3.11/site-packages (from unstructured-client->unstructured[local-inference]) (2.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in ./.venv/lib/python3.11/site-packages (from unstructured-client->unstructured[local-inference]) (1.0.0)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.11/site-packages (from httpx>=0.27.0->unstructured-client->unstructured[local-inference]) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx>=0.27.0->unstructured-client->unstructured[local-inference]) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured[local-inference]) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.11/site-packages (from pydantic>=2.11.2->unstructured-client->unstructured[local-inference]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.11/site-packages (from pydantic>=2.11.2->unstructured-client->unstructured[local-inference]) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.11/site-packages (from pydantic>=2.11.2->unstructured-client->unstructured[local-inference]) (0.4.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.11/site-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured[local-inference]) (1.3.1)\n",
      "Downloading unstructured-0.17.2-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading onnx-1.18.0-cp311-cp311-macosx_12_0_universal2.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.22.0-cp311-cp311-macosx_13_0_universal2.whl (34.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.31.1-cp39-abi3-macosx_10_9_universal2.whl (425 kB)\n",
      "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
      "Downloading lxml-5.4.0-cp311-cp311-macosx_10_9_universal2.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
      "Downloading unstructured_inference-1.0.5-py3-none-any.whl (48 kB)\n",
      "Downloading opencv_python-4.11.0.86-cp37-abi3-macosx_13_0_arm64.whl (37.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading unstructured.pytesseract-0.3.15-py3-none-any.whl (14 kB)\n",
      "Downloading XlsxWriter-3.2.3-py3-none-any.whl (169 kB)\n",
      "Downloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
      "Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Downloading effdet-0.4.1-py3-none-any.whl (112 kB)\n",
      "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Downloading pycocotools-2.0.10-cp311-cp311-macosx_10_9_universal2.whl (155 kB)\n",
      "Downloading timm-1.0.15-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading google_cloud_vision-3.10.2-py3-none-any.whl (527 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.9/527.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_api_core-2.25.1-py3-none-any.whl (160 kB)\n",
      "Downloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading grpcio-1.73.0-cp311-cp311-macosx_11_0_universal2.whl (10.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading grpcio_status-1.73.0-py3-none-any.whl (14 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "Downloading markdown-3.8-py3-none-any.whl (106 kB)\n",
      "Downloading matplotlib-3.10.3-cp311-cp311-macosx_11_0_arm64.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.2-cp311-cp311-macosx_11_0_arm64.whl (254 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.58.4-cp311-cp311-macosx_10_9_universal2.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp311-cp311-macosx_11_0_arm64.whl (65 kB)\n",
      "Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Downloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cryptography-45.0.4-cp311-abi3-macosx_10_9_universal2.whl (7.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pi_heif-0.22.0-cp311-cp311-macosx_14_0_arm64.whl (559 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m559.8/559.8 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pikepdf-9.8.1-cp311-cp311-macosx_14_0_arm64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Downloading wrapt-1.17.2-cp311-cp311-macosx_11_0_arm64.whl (38 kB)\n",
      "Downloading pypandoc-1.15-py3-none-any.whl (21 kB)\n",
      "Downloading pypdfium2-4.30.1-py3-none-macosx_11_0_arm64.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_iso639-2025.2.18-py3-none-any.whl (167 kB)\n",
      "Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading python_oxmsg-0.0.2-py3-none-any.whl (31 kB)\n",
      "Downloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
      "Downloading rapidfuzz-3.13.0-cp311-cp311-macosx_11_0_arm64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.22.1-cp311-cp311-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.7.1-cp311-none-macosx_11_0_arm64.whl (68.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading unstructured_client-0.36.0-py3-none-any.whl (195 kB)\n",
      "Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Downloading xlrd-2.0.2-py2.py3-none-any.whl (96 kB)\n",
      "Building wheels for collected packages: antlr4-python3-runtime, langdetect\n",
      "\u001b[33m  DEPRECATION: Building 'antlr4-python3-runtime' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'antlr4-python3-runtime'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144592 sha256=1cc08efae6fe97095e793ddfc3ade044bfe8c40de354b322949bb13cd3df42fd\n",
      "  Stored in directory: /Users/giacomo/Library/Caches/pip/wheels/1a/97/32/461f837398029ad76911109f07047fde1d7b661a147c7c56d1\n",
      "\u001b[33m  DEPRECATION: Building 'langdetect' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'langdetect'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993331 sha256=21adf90b8338e62d31994bd11a25fa7a329b78f5d42063b369d9b885f28ca8de\n",
      "  Stored in directory: /Users/giacomo/Library/Caches/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
      "Successfully built antlr4-python3-runtime langdetect\n",
      "Installing collected packages: flatbuffers, filetype, antlr4-python3-runtime, XlsxWriter, xlrd, wrapt, unstructured.pytesseract, rapidfuzz, python-multipart, python-magic, python-iso639, pypdfium2, pyparsing, pypandoc, pycocotools, pyasn1, protobuf, pi-heif, pdf2image, opencv-python, omegaconf, olefile, markdown, lxml, langdetect, kiwisolver, humanfriendly, html5lib, grpcio, fonttools, et-xmlfile, emoji, cycler, contourpy, chardet, cachetools, aiofiles, torch, rsa, python-pptx, python-oxmsg, python-docx, pyasn1-modules, proto-plus, openpyxl, onnx, matplotlib, googleapis-common-protos, Deprecated, cryptography, coloredlogs, unstructured-client, torchvision, pikepdf, pdfminer.six, onnxruntime, grpcio-status, google-auth, accelerate, unstructured, timm, google-api-core, unstructured-inference, effdet, google-cloud-vision\n",
      "\u001b[2K  Attempting uninstall: cachetools91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34/65\u001b[0m [chardet]s]hon]\n",
      "\u001b[2K    Found existing installation: cachetools 6.0.0━━━━━━━━━━━━━\u001b[0m \u001b[32m34/65\u001b[0m [chardet]\n",
      "\u001b[2K    Uninstalling cachetools-6.0.0:\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34/65\u001b[0m [chardet]\n",
      "\u001b[2K      Successfully uninstalled cachetools-6.0.0━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34/65\u001b[0m [chardet]\n",
      "\u001b[2K  Attempting uninstall: torch[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34/65\u001b[0m [chardet]\n",
      "\u001b[2K    Found existing installation: torch 2.7.0━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34/65\u001b[0m [chardet]\n",
      "\u001b[2K    Uninstalling torch-2.7.0:━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37/65\u001b[0m [torch]\n",
      "\u001b[2K      Successfully uninstalled torch-2.7.090m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37/65\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65/65\u001b[0m [google-cloud-vision]d-vision]d]client]\n",
      "\u001b[1A\u001b[2KSuccessfully installed Deprecated-1.2.18 XlsxWriter-3.2.3 accelerate-1.7.0 aiofiles-24.1.0 antlr4-python3-runtime-4.9.3 cachetools-5.5.2 chardet-5.2.0 coloredlogs-15.0.1 contourpy-1.3.2 cryptography-45.0.4 cycler-0.12.1 effdet-0.4.1 emoji-2.14.1 et-xmlfile-2.0.0 filetype-1.2.0 flatbuffers-25.2.10 fonttools-4.58.4 google-api-core-2.25.1 google-auth-2.40.3 google-cloud-vision-3.10.2 googleapis-common-protos-1.70.0 grpcio-1.73.0 grpcio-status-1.73.0 html5lib-1.1 humanfriendly-10.0 kiwisolver-1.4.8 langdetect-1.0.9 lxml-5.4.0 markdown-3.8 matplotlib-3.10.3 olefile-0.47 omegaconf-2.3.0 onnx-1.18.0 onnxruntime-1.22.0 opencv-python-4.11.0.86 openpyxl-3.1.5 pdf2image-1.17.0 pdfminer.six-20250506 pi-heif-0.22.0 pikepdf-9.8.1 proto-plus-1.26.1 protobuf-6.31.1 pyasn1-0.6.1 pyasn1-modules-0.4.2 pycocotools-2.0.10 pypandoc-1.15 pyparsing-3.2.3 pypdfium2-4.30.1 python-docx-1.1.2 python-iso639-2025.2.18 python-magic-0.4.27 python-multipart-0.0.20 python-oxmsg-0.0.2 python-pptx-1.0.2 rapidfuzz-3.13.0 rsa-4.9.1 timm-1.0.15 torch-2.7.1 torchvision-0.22.1 unstructured-0.17.2 unstructured-client-0.36.0 unstructured-inference-1.0.5 unstructured.pytesseract-0.3.15 wrapt-1.17.2 xlrd-2.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"unstructured[local-inference]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99fbb02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF\n",
      "  Downloading pymupdf-1.26.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (3.4 kB)\n",
      "Downloading pymupdf-1.26.1-cp39-abi3-macosx_11_0_arm64.whl (22.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.4/22.4 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyMuPDF\n",
      "Successfully installed PyMuPDF-1.26.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be8312e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giacomo/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Strategy: fast ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading averaged_perceptron_tagger_eng: <urlopen\n",
      "[nltk_data]     error [SSL: CERTIFICATE_VERIFY_FAILED] certificate\n",
      "[nltk_data]     verify failed: unable to get local issuer certificate\n",
      "[nltk_data]     (_ssl.c:1006)>\n",
      "[nltk_data] Error loading punkt_tab: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1006)>\n"
     ]
    }
   ],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.cleaners.core import clean # For cleaning text\n",
    "import pandas as pd\n",
    "import io # For handling HTML string if needed\n",
    "\n",
    "def pdf_to_markdown_elements(pdf_path, strategy=\"fast\"):\n",
    "    \"\"\"\n",
    "    Parses a PDF and attempts to convert its content to Markdown-like elements.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        strategy (str): The partitioning strategy to use.\n",
    "                        Common options: \"fast\", \"ocr_only\", \"hi_res\".\n",
    "                        \"fast\" is good for digitally-born PDFs.\n",
    "                        \"ocr_only\" for scanned PDFs (requires Tesseract).\n",
    "                        \"hi_res\" for complex layouts, uses model-based detection (like Detectron2 if installed).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of strings, where each string is a Markdown-formatted element.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # The infer_table_structure=True is crucial for getting better table data\n",
    "        # For hi_res strategy, layout_kwargs can be used to pass model-specific params\n",
    "        elements = partition_pdf(\n",
    "            filename=pdf_path,\n",
    "            strategy=strategy,\n",
    "            infer_table_structure=True, # Tries to parse table structure\n",
    "            # For hi_res, if you have models like Detectron2 or YOLOX installed:\n",
    "            # model_name=\"yolox\" # or \"detectron2_onnx\"\n",
    "            # ocr_languages=\"eng\" # Example if using OCR within hi_res\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error partitioning PDF {pdf_path}: {e}\")\n",
    "        return [f\"<!-- Error processing PDF: {e} -->\"]\n",
    "\n",
    "    markdown_elements = []\n",
    "    for el_index, element in enumerate(elements):\n",
    "        element_type = str(type(element)).lower() # Get a string representation of the element type\n",
    "\n",
    "        if \"table\" in element_type:\n",
    "            # Tables are often returned as HTML in element.metadata.text_as_html\n",
    "            # Or sometimes the raw text in element.text is a semi-structured representation\n",
    "            html_table = getattr(element.metadata, 'text_as_html', None)\n",
    "            if html_table:\n",
    "                try:\n",
    "                    # Use pandas to read the HTML table and convert to Markdown\n",
    "                    # Ensure you have 'lxml' installed (pip install lxml) for pandas.read_html\n",
    "                    df_list = pd.read_html(io.StringIO(html_table))\n",
    "                    if df_list:\n",
    "                        df = df_list[0] # Assuming the first table found is the one\n",
    "                        # Clean up NaN values before converting to markdown\n",
    "                        df_cleaned = df.fillna('')\n",
    "                        markdown_table = df_cleaned.to_markdown(index=False)\n",
    "                        markdown_elements.append(f\"## Table {el_index + 1}\\n\\n{markdown_table}\\n\")\n",
    "                    else:\n",
    "                        markdown_elements.append(f\"<!-- Table {el_index + 1} found but could not be parsed from HTML -->\\n{clean(element.text, extra_whitespace=True)}\\n\")\n",
    "                except Exception as e:\n",
    "                    # Fallback if pandas fails\n",
    "                    markdown_elements.append(f\"<!-- Table {el_index + 1} (Pandas Error: {e}) -->\\n{clean(element.text, extra_whitespace=True)}\\n\") # Fallback to text\n",
    "            else:\n",
    "                # If no HTML, just use the text content (might be less structured)\n",
    "                markdown_elements.append(f\"## Table {el_index + 1} (Raw Text)\\n\\n```text\\n{clean(element.text, extra_whitespace=True)}\\n```\\n\")\n",
    "        elif \"title\" in element_type:\n",
    "            level = getattr(element.metadata, 'category_depth', None)\n",
    "            if level is not None and 0 <= level <= 5: # Typical heading levels\n",
    "                markdown_elements.append(f\"{'#' * (level + 1)} {clean(element.text, extra_whitespace=True)}\\n\")\n",
    "            else:\n",
    "                markdown_elements.append(f\"# {clean(element.text, extra_whitespace=True)}\\n\") # Default to H1\n",
    "        elif \"listitem\" in element_type:\n",
    "            # This is a simplification; true list structure requires more context\n",
    "            markdown_elements.append(f\"* {clean(element.text, extra_whitespace=True)}\\n\")\n",
    "        else: # NarrativeText, Address, etc.\n",
    "            cleaned_text = clean(element.text, extra_whitespace=True, bullets=False, trailing_punctuation=False)\n",
    "            if cleaned_text.strip(): # Add if not just whitespace\n",
    "                markdown_elements.append(f\"{cleaned_text}\\n\")\n",
    "\n",
    "    return markdown_elements\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a dummy PDF for testing if you don't have one\n",
    "    # For a real scenario, replace \"your_document.pdf\" with the actual path\n",
    "    pdf_file_path = \"docs/BURP_n.177_del_17112008.pdf\" # REPLACE THIS WITH YOUR PDF PATH\n",
    "\n",
    "    # --- You might need to install lxml for pandas.read_html ---\n",
    "    # pip install lxml\n",
    "\n",
    "    # --- If using \"ocr_only\" or \"hi_res\" with OCR, ensure Tesseract is installed ---\n",
    "    # On macOS: brew install tesseract tesseract-lang\n",
    "    # On Linux: sudo apt-get install tesseract-ocr tesseract-ocr-eng (or your language)\n",
    "    # Also, make sure TESSDATA_PREFIX is set if needed.\n",
    "\n",
    "    print(f\"--- Strategy: fast ---\")\n",
    "    markdown_output_fast = pdf_to_markdown_elements(pdf_file_path, strategy=\"fast\")\n",
    "    for item in markdown_output_fast:\n",
    "        print(item)\n",
    "\n",
    "    # To use \"hi_res\", you might need to install Detectron2 or other layout models\n",
    "    # pip install \"unstructured[all-models]\" (this is large) or specific ones like\n",
    "    # pip install \"unstructured.PaddleOCR\" \"unstructured.pytesseract\" (for hi_res OCR)\n",
    "    # pip install \"layoutparser\" \"detectron2@git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2\"\n",
    "    # (Detectron2 installation can be tricky, especially on non-Linux systems)\n",
    "    #\n",
    "    # print(f\"\\n--- Strategy: hi_res (if models are installed) ---\")\n",
    "    # markdown_output_hi_res = pdf_to_markdown_elements(pdf_file_path, strategy=\"hi_res\")\n",
    "    # for item in markdown_output_hi_res:\n",
    "    #     print(item)\n",
    "\n",
    "    # Combine all elements into a single markdown string\n",
    "    final_markdown_fast = \"\\n\".join(markdown_output_fast)\n",
    "    # print(\"\\n\\n--- Combined Markdown (fast) ---\")\n",
    "    # print(final_markdown_fast)\n",
    "\n",
    "    # You can save this to a .md file\n",
    "    # with open(\"output_fast.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    #     f.write(final_markdown_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7305401a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/giacomo/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ssl\n",
    "import nltk\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6c104bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: docs/Legge regionale n_37_2014 artt. 20-21-22.pdf\n",
      "Processing file: docs/Direttiva 2014_25_UE.pdf\n",
      "Processing file: docs/Direttiva 2014_23_UE.pdf\n",
      "Processing file: docs/Decreto Legislativo 7 marzo 2005_agg_L_147_2013.pdf\n",
      "Processing file: docs/L. 27 Dicembre 2006 n.296 (Finanziaria 2007).pdf\n",
      "Processing file: docs/L. 23 Dicembre 2000 n.388 (Finanziaria 2001).pdf\n",
      "Processing file: docs/dPR 5 ottobre 2010_207_agg_DM_infrastrutture_24apr2014.pdf\n",
      "Processing file: docs/Direttiva 2014_24_UE.pdf\n",
      "Processing file: docs/D.Lgs. 50_2016.pdf\n",
      "Processing file: docs/DGR_17_2024_01_22_signed_signed.pdf\n",
      "Processing file: docs/Decreto legislativo 12 aprile  2006_163_agg_DL_24apr2014_n_66.pdf\n",
      "Processing file: docs/L. 23 Dicembre 1999 n.488 (Finanziaria 2000).pdf\n",
      "Processing file: docs/BURP_n.177_del_17112008.pdf\n",
      "Processing file: docs/DELIBERAZIONE DELLA GIUNTA REGIONALE 21 marzo 2017 n_354.pdf\n",
      "Processing file: docs/Dir.1999 93 CE del Parlamento Europeo e del Consiglio.pdf\n",
      "length of list_docs_md: 15\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import os\n",
    "\n",
    "def clean_text_for_markdown(text):\n",
    "    \"\"\"Cleans text for better Markdown display, handling multiple newlines.\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    # Replace multiple newlines with a single one, but preserve paragraph breaks (double newlines)\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text) # Consolidate paragraph breaks\n",
    "    text = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text) # Replace single newlines (likely line breaks within a paragraph) with space\n",
    "    return text.strip()\n",
    "\n",
    "def pymupdf_to_markdown(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text and tables from a PDF using PyMuPDF and formats them for Markdown.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        str: A string containing the PDF content formatted in Markdown.\n",
    "    \"\"\"\n",
    "    doc = None\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "    except Exception as e:\n",
    "        return f\"<!-- Error opening PDF {pdf_path}: {e} -->\"\n",
    "\n",
    "    markdown_content = []\n",
    "\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        markdown_content.append(f\"\\n## Page {page_num + 1}\\n\\n\")\n",
    "\n",
    "        # 1. Find and extract tables first\n",
    "        table_objects = page.find_tables()\n",
    "        page_tables_bboxes = [] # Store bboxes of tables found on this page\n",
    "\n",
    "        if table_objects.tables: # Check if TableFinder found any tables\n",
    "            markdown_content.append(f\"### Tables on Page {page_num + 1}\\n\")\n",
    "            for i, tab in enumerate(table_objects.tables):\n",
    "                page_tables_bboxes.append(fitz.Rect(tab.bbox)) # Store the bounding box\n",
    "                table_data = tab.extract() # Extracts as list of lists\n",
    "\n",
    "                if not table_data:\n",
    "                    markdown_content.append(f\"<!-- Table {i+1} detected but no data extracted. -->\\n\")\n",
    "                    continue\n",
    "\n",
    "                # Convert table data to Markdown\n",
    "                # Header\n",
    "                header = table_data[0]\n",
    "                # Ensure all header cells are strings, replace None with empty string\n",
    "                header_md = \"| \" + \" | \".join([str(cell).replace(\"\\n\", \" \").strip() if cell is not None else \"\" for cell in header]) + \" |\"\n",
    "                separator_md = \"| \" + \" | \".join([\"---\"] * len(header)) + \" |\"\n",
    "                \n",
    "                rows_md = []\n",
    "                for row in table_data[1:]:\n",
    "                    # Ensure all row cells are strings, replace None with empty string\n",
    "                    # Replace internal newlines in cells with a space for basic markdown\n",
    "                    row_cells = [str(cell).replace(\"\\n\", \" \").strip() if cell is not None else \"\" for cell in row]\n",
    "                    rows_md.append(\"| \" + \" | \".join(row_cells) + \" |\")\n",
    "                \n",
    "                markdown_content.append(header_md)\n",
    "                markdown_content.append(separator_md)\n",
    "                markdown_content.extend(rows_md)\n",
    "                markdown_content.append(\"\\n\") # Add a blank line after the table\n",
    "\n",
    "        # 2. Extract text blocks, excluding those within table bounding boxes\n",
    "        text_blocks = page.get_text(\"blocks\", sort=True) # Get text blocks, sorted by y-coordinate\n",
    "        \n",
    "        page_text_content = []\n",
    "        for block in text_blocks:\n",
    "            x0, y0, x1, y1, text_content, block_no, block_type = block\n",
    "            block_rect = fitz.Rect(x0, y0, x1, y1)\n",
    "            \n",
    "            # Check if this block is inside any detected table\n",
    "            is_in_table = False\n",
    "            for table_bbox in page_tables_bboxes:\n",
    "                # Check for significant overlap or if block is contained\n",
    "                # A simple check: if the intersection area is a large portion of the block's area\n",
    "                intersection = fitz.Rect(block_rect)\n",
    "                intersection.intersect(table_bbox)\n",
    "                if not intersection.is_empty and intersection.width * intersection.height > 0.5 * block_rect.width * block_rect.height:\n",
    "                    is_in_table = True\n",
    "                    break\n",
    "            \n",
    "            if not is_in_table:\n",
    "                cleaned_block_text = clean_text_for_markdown(text_content)\n",
    "                if cleaned_block_text: # Add only if there's actual text after cleaning\n",
    "                    page_text_content.append(cleaned_block_text)\n",
    "\n",
    "        if page_text_content:\n",
    "            markdown_content.append(f\"### Text on Page {page_num + 1}\\n\")\n",
    "            markdown_content.append(\"\\n\\n\".join(page_text_content)) # Join text blocks with double newline for paragraphs\n",
    "            markdown_content.append(\"\\n\")\n",
    "\n",
    "\n",
    "    if doc:\n",
    "        doc.close()\n",
    "    \n",
    "    return \"\\n\".join(markdown_content)\n",
    "\n",
    "# --- Example Usage ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"docs\"  # Folder containing PDF files\n",
    "    list_docs_md = []\n",
    "\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"The folder '{folder_path}' does not exist. Please provide a valid folder path.\")\n",
    "    else:\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith(\".pdf\"):\n",
    "                pdf_file_path = os.path.join(folder_path, file_name)\n",
    "                print(f\"Processing file: {pdf_file_path}\")\n",
    "                markdown_result = pymupdf_to_markdown(pdf_file_path)\n",
    "                list_docs_md.append(markdown_result)\n",
    "\n",
    "print(\"length of list_docs_md:\", len(list_docs_md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63651e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Installing collected packages: pypdf2\n",
      "Successfully installed pypdf2-3.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pypdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3b7ccc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: docs/Legge regionale n_37_2014 artt. 20-21-22.pdf\n",
      "Processing file: docs/Direttiva 2014_25_UE.pdf\n",
      "Processing file: docs/Direttiva 2014_23_UE.pdf\n",
      "Processing file: docs/Decreto Legislativo 7 marzo 2005_agg_L_147_2013.pdf\n",
      "Processing file: docs/L. 27 Dicembre 2006 n.296 (Finanziaria 2007).pdf\n",
      "Processing file: docs/L. 23 Dicembre 2000 n.388 (Finanziaria 2001).pdf\n",
      "Processing file: docs/dPR 5 ottobre 2010_207_agg_DM_infrastrutture_24apr2014.pdf\n",
      "Processing file: docs/Direttiva 2014_24_UE.pdf\n",
      "Processing file: docs/D.Lgs. 50_2016.pdf\n",
      "Processing file: docs/DGR_17_2024_01_22_signed_signed.pdf\n",
      "Processing file: docs/Decreto legislativo 12 aprile  2006_163_agg_DL_24apr2014_n_66.pdf\n",
      "Processing file: docs/L. 23 Dicembre 1999 n.488 (Finanziaria 2000).pdf\n",
      "Processing file: docs/BURP_n.177_del_17112008.pdf\n",
      "Processing file: docs/DELIBERAZIONE DELLA GIUNTA REGIONALE 21 marzo 2017 n_354.pdf\n",
      "Processing file: docs/Dir.1999 93 CE del Parlamento Europeo e del Consiglio.pdf\n",
      "length of list_docs_md_pypdf2: 15\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "import os\n",
    "\n",
    "def clean_text_for_markdown_pypdf2(text):\n",
    "    \"\"\"Cleans text for better Markdown display, handling multiple newlines.\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    # Replace multiple newlines with a single one, but preserve paragraph breaks (double newlines)\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)  # Consolidate paragraph breaks\n",
    "    text = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text)  # Replace single newlines with space\n",
    "    return text.strip()\n",
    "\n",
    "def pypdf2_to_markdown(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF using PyPDF2 and formats it for Markdown.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        str: A string containing the PDF content formatted in Markdown.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "    except Exception as e:\n",
    "        return f\"<!-- Error opening PDF {pdf_path}: {e} -->\"\n",
    "\n",
    "    markdown_content = []\n",
    "\n",
    "    for page_num, page in enumerate(reader.pages):\n",
    "        markdown_content.append(f\"\\n## Page {page_num + 1}\\n\\n\")\n",
    "\n",
    "        try:\n",
    "            # Extract text from the page\n",
    "            page_text = page.extract_text()\n",
    "            cleaned_text = clean_text_for_markdown_pypdf2(page_text)\n",
    "            if cleaned_text:\n",
    "                markdown_content.append(f\"### Text on Page {page_num + 1}\\n\")\n",
    "                markdown_content.append(cleaned_text)\n",
    "                markdown_content.append(\"\\n\")\n",
    "            else:\n",
    "                markdown_content.append(f\"<!-- No text found on Page {page_num + 1} -->\\n\")\n",
    "        except Exception as e:\n",
    "            markdown_content.append(f\"<!-- Error extracting text from Page {page_num + 1}: {e} -->\\n\")\n",
    "\n",
    "    return \"\\n\".join(markdown_content)\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"docs\"  # Folder containing PDF files\n",
    "    list_docs_md_pypdf2 = []\n",
    "\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"The folder '{folder_path}' does not exist. Please provide a valid folder path.\")\n",
    "    else:\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith(\".pdf\"):\n",
    "                pdf_file_path = os.path.join(folder_path, file_name)\n",
    "                print(f\"Processing file: {pdf_file_path}\")\n",
    "                markdown_result = pypdf2_to_markdown(pdf_file_path)\n",
    "                list_docs_md_pypdf2.append(markdown_result)\n",
    "\n",
    "print(\"length of list_docs_md_pypdf2:\", len(list_docs_md_pypdf2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "814431c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n## Page 1\\n\\n\\n### Text on Page 1\\n\\nREPUBBLICA ITALIANA ANNO XLV BARI, 8 AGOSTO 2014 n. 109BOLLETTINO UFFICIALE della Regione Puglia Leggi e regolamenti regionali VOLUME PRIMO 2014.08. 19  09:09:08 +02'00'\\n\\n\\n\\n## Page 2\\n\\n\\n### Text on Page 2\\n\\nIl Bollettino Ufficiale della Regione Puglia si pubblica con frequenza infrasettimanale ed è diviso in due parti. Nella parte I sono pubblicati: a) sentenze ed ordinanze della Corte Costituzionale riguardanti leggi della Regione Puglia; b) ricorsi e sentenze di Organi giurisdizionali che prevedono un coinvolgimento della Regione Puglia;c) leggi e regolamenti regionali;d) deliberazioni del Consiglio Regionale riguardanti la convalida degli eletti;e) atti e circolari aventi rilevanza esterna;f) comunicati ufficiali emanati dal Presidente della Regione e dal Presidente del Consiglio Regionale;g) atti relativi all’elezione dell’Ufficio di Presidenza dell’ Assemblea, della Giunta regionale, delle Commissioni permanenti e loro eventuali dimissioni; h) deliberazioni, atti e provvedimenti generali attuativi delle direttive ed applicativi dei regolamenti della Comunità Europea;i) disegni di legge ai sensi dell’art. 8 della L.R. n. 19/97;j) lo Statuto regionale e le sue modificazioni;k) richieste di referendum con relativi risultati;l) piano di sviluppo regionale con aggiornamenti o modifiche. Nella parte II sono pubblicati: a) decreti ed ordinanze del Presidente della Giunta regionale; b) deliberazioni della Giunta regionale;c) determinazioni dirigenziali;d) decreti ed ordinanze del Presidente della Giunta regionale in veste di Commissario delegato;e) atti del Difensore Civico regionale come previsto da norme regionali o su disposizioni del Presidente o della Giunta;f) atti degli Enti Locali;g) deliberazioni del Consiglio Regionale;h) statuti di enti locali;i) concorsi;j) avvisi di gara;k) annunci legali;l) avvisi;m) rettifiche;n) atti di organi non regionali, di altri enti o amministrazioni, aventi particolare rilievo e la cui pubblicazione non è prescr itta.\\n\\n\\n\\n## Page 3\\n\\n\\n### Text on Page 3\\n\\nBollettino Ufficiale della Regione Puglia ‐ n. 109 dell’8‐08‐2014 ‐ volume primo 27895 “Avviso per i redattori e per gli Enti:  Il Bollettino Ufficiale della Regione Puglia si attiene alle regole della Legge 150/2000 per la semplificazione del linguaggio e per la facilitazione dell'accesso dei cittadini alla comprensione degli atti della Pubblica Amministrazione. Tutti i redattori e gli Enti inserzionisti sono tenuti ad evitare sigle, acronimi, abbreviazioni,almeno nei titoli di testa dei provvedimenti”. PARTE PRIMA Leggi e regolamenti regionali LEGGE REGIONALE 1 agosto 2014, n. 37 “Assestamento e prima variazione al bilancio di previsione per l’esercizio finanziario 2014”. Pag. 27896 VOLUME PRIMOSOMMARIO\\n\\n\\n\\n## Page 4\\n\\n\\n### Text on Page 4\\n\\n4. La Giunta regionale, su proposta del Presidente o dell’ Assessore dele‐ gato, verificata la consistenza del fondo, provvede ad assegnare, ogni seimesi, il 95 per cento delle risorse disponibili ai soggetti di cui al comma 1 edestina il restante 5 per cento alle azioni di sensibilizzazione di cui al comma3. 5. Con apposito regolamento la Giunta regionale definisce le modalità per l’acquisizione delle sottoscrizioni volontarie e per la gestione delle risorsedel fondo”. 2. Per le finalità di cui all’articolo 10 bis della l.r. 7/2006, come introdotto dal comma 1 del presente articolo, è istituito, nel bilancio regionale autonomo, nel‐l’ambito della U.P .B. 02.03.02, il capitolo di spesa n. 212050, denominato “Fondoregionale contro l’usura. Finanziamento alle fondazioni per le finalità di cui all’ar‐ticolo 10 bis della l.r. 7/2006”, con una dotazione finanziaria per l’esercizio finan‐ziario 2014, in termini di competenza e cassa, di euro 150 mila. Per gli esercizifinanziari successivi si provvede in sede di approvazione delle rispettive leggi di bi‐lancio. Art. 17 Ottemperanza alla Sentenza del Consiglio di Stato n. 1755/2013 1. Al fine di ottemperare alla Sentenza del Consiglio di Stato n. 1755/2013, dalla quale scaturisce un debito della Regione Puglia nei confronti della società Ferroviedel Sud Est e Servizi Automobilistici s.r.l., a saldo pari a euro 72.981.116,90, è isti‐tuito nel bilancio regionale autonomo, nell’ambito della U.P .B. 03.04.02, il capitolodi spesa n. 551015, denominato “Spese connesse alla ottemperanza alla Sentenzadel Consiglio di Stato n. 1755/2013“, con una dotazione finanziaria, in termini dicompetenza e cassa, di euro 20 milioni per l’esercizio finanziario 2014 ed euro26.490.558,45 per ciascuno degli esercizi 2015 e 2016. 2. Eventuali recuperi a qualsiasi titolo, da eseguire nei confronti della società Ferrovie del Sud Est, possono essere effettuati secondo le disposizioni di cui alcomma 3 dell’articolo 31 della legge regionale 31 ottobre 2002, n. 18 ( Testo unico sulla disciplina del trasporto pubblico locale ). Art. 18 Partecipazione della Regione Puglia a Padiglione Italia ‐ Expo 2015 1. Al fine di assicurare la partecipazione della Regione Puglia alla manifesta‐ zione EXPO 2015 è istituito nel bilancio regionale autonomo, nell’ambito dellaU.P .B. 01.00.01, il capitolo di spesa n. 111015, denominato “Spese per la parteci‐pazione della Regione Puglia al Padiglione Italia ‐ Expo 2015”, con una dotazionefinanziaria per l’esercizio finanziario 2014, in termini di competenza e cassa, di euro366 mila. Art. 19 Spese per attività connesse alla realizzazione del P .S.R. 2007‐2013  svolte in regime di convenzione 1. Al fine di assicurare la copertura degli oneri derivanti dalla convenzione di cui alla deliberazione della Giunta regionale 7 maggio 2009, n. 751 ( Società “in house” Innovapuglia s.p.a. Convenzione per la disciplina di fornitura dei servizi. Af‐fidamento servizi nell’ambito dei programmi comunitari 2007‐2013 ), per le attività connesse alla progettazione e realizzazione di un Sistema informativo per il moni‐toraggio degli interventi finanziati nell’ambito del Programma di Sviluppo Rurale(P .S.R.) per la Puglia 2007‐2013, relativamente alla parte non ammissibile a rendi‐contazione a carico del predetto programma, è istituito nel bilancio regionale au‐tonomo, nell’ambito della U.P .B. 01.00.02, il capitolo di spesa n. 1150815,denominato “Spese per la progettazione e realizzazione del Sistema di monitorag‐gio del P .S.R. Puglia 2007‐2013, finanziato ai sensi della deliberazione della Giuntaregionale 751/2009”, con una dotazione finanziaria per l’esercizio finanziario 2014,in termini di competenza e cassa, di euro 850 mila. Art. 20  Soggetto aggregatore della Regione Puglia 1. La Regione Puglia, al fine del perseguimento degli obiettivi di finanza pub‐ blica e di trasparenza, regolarità ed economicità della gestione dei contratti pub‐Bollettino Ufficiale della Regione Puglia ‐ n. 109 dell’8‐08‐2014 ‐ volume primo 27902\\n\\n\\n\\n## Page 5\\n\\n\\n### Text on Page 5\\n\\nblici, promuove e sviluppa, nel rispetto della normativa nazionale, il processo di razionalizzazione dell’acquisizione di lavori, beni e servizi delle amministrazioni edegli enti aventi sede nel territorio regionale attraverso il ricorso alla centrale dicommittenza regionale. 2. Ai fini del perseguimento degli obiettivi di cui al comma 1 e in attuazione dell’articolo 9, comma 5, del d.l. 66/2014, convertito, con modificazioni, dalla l.89/2014, la Regione designa la società in house InnovaPuglia spa Soggetto aggre‐gatore della regione Puglia, nella sua qualità di centrale di committenza, costituitaai sensi del comma 455 dell’articolo 1 della legge 27 dicembre 2006, n. 296 ( Dispo‐ sizioni per la formazione del bilancio annuale e pluriennale dello Stato ), e di centrale di acquisto territoriale ai sensi dell’articolo 33 del d.lgs. 163/2006. 3. Per lo svolgimento delle funzioni e dei compiti di cui ai commi 1 e 2 il Sog‐ getto aggregatore, in particolare, svolge le seguenti attività: a) stipula convenzioni quadro di cui all’articolo 26 della legge 23 dicembre 1999, n. 488 ( Disposizioni per la formazione del bilancio annuale e plu‐ riennale dello Stato ), e accordi quadro di cui all’articolo 59 del d.lgs. 163/2006; b) gestisce sistemi dinamici di acquisizione ai sensi dell’articolo 60 del d.lgs. 163/2006; c) gestisce le procedure di gara, svolgendo le attività e i servizi di stazione unica appaltante ai sensi del decreto Presidente del Consiglio dei ministri30 giugno 2011 ( Indizione Stazione Unica Appaltante, in attuazione del‐ l’articolo 13 della legge 13 agosto 2010, n. 136 ‐ Piano straordinario con‐tro le mafie ), procedendo all’aggiudicazione del contratto; d) cura la gestione dell’albo dei fornitori “on line” di cui al regolamento re‐ gionale 11 novembre 2008, n. 22; e) assicura lo svolgimento delle attività di committenza ausiliarie ai sensi della direttiva 2014/24/UE del Parlamento europeo e del Consiglio del26 febbraio 2014 sugli appalti pubblici; f) assicura la continuità di esercizio, lo sviluppo e la promozione del servizio telematico denominato EmPulia. 4. Il Soggetto aggregatore fornisce le attività di centralizzazione delle commit‐ tenze e quelle ausiliarie, come elencate al comma 3, in favore della Regione e delleaziende ed enti del SSR, i quali sono tenuti a ricorrere al Soggetto aggregatore re‐ gionale per la acquisizione di lavori, beni e servizi, secondo le modalità individuatenegli atti della Programmazione regionale di cui all’articolo 21, fatte salve le speci‐fiche disposizioni nazionali che consentono il ricorso ad altre centrali di commit‐tenza e l’utilizzo di altri strumenti telematici. 5. Il Soggetto aggregatore può svolgere, previa stipulazione di apposita con‐ venzione, le proprie attività in favore dei seguenti soggetti: a) enti e agenzie regionali;b) enti locali, nonché loro consorzi, unioni o associazioni;c) eventuali ulteriori soggetti interessati di cui all’articolo 32 del d.lgs. 163/2006. 6. Con deliberazione di Giunta, la Regione Puglia disciplina le modalità opera‐ tive in base alle quali le strutture amministrative regionali usufruiscono delle atti‐vità del Soggetto aggregatore secondo quanto previsto dal comma 4, approva loschema della convenzione di cui al comma 5 e individua le modalità per la coper‐tura delle spese e dei costi di funzionamento della centrale di committenza, ela‐borando un piano tariffario per l’utilizzo dei servizi del Soggetto aggregatore,distinguendo tra: a) adesione alla centrale di committenza per l’acquisizione di beni e servizi attraverso il ricorso a convenzioni quadro di cui all’articolo 26 della l.488/1999, accordi quadro di cui all’articolo 59 del d.lgs. 163/2006 e si‐stemi dinamici di acquisizione ai sensi dell’articolo 60 del d.lgs. 163/2006; b) adesione alla centrale di committenza per lo svolgimento delle funzioni di stazione unica appaltante ai sensi del d.p.c.m. 30 giugno 2011; c) accesso all’albo dei fornitori on line di cui al r.r. 22/2008;d) prestazione delle attività di committenza ausiliarie ai sensi della direttiva 2014/24/UE del Parlamento europeo e del Consiglio del 26 febbraio 2014sugli appalti pubblici; e) utilizzo del servizio telematico denominato EmPulia. 7. Sono abrogati l’articolo 54 della legge regionale 25 febbraio 2010, n. 4 (Norme urgenti in materia di sanità e servizi sociali ) e il comma 2 dell’articolo 42 della l.r.45/2012. Bollettino Ufficiale della Regione Puglia ‐ n. 109 dell’8‐08‐2014 ‐ volume primo 27903\\n\\n\\n\\n## Page 6\\n\\n\\n### Text on Page 6\\n\\nArt. 21 Programmazione regionale delle acquisizioni di lavori, beni e servizi con ricorso al Soggetto aggregatore 1. Fatti salvi specifici obblighi di legge nazionale, con apposita deliberazione di Giunta regionale di approvazione del Piano regionale delle attività negoziali, daadottarsi entro il 31 dicembre di ogni anno, sulla base dell’analisi svolta dalla dire‐zione dell’ Area politiche per la promozione della salute, sono individuati le cate‐gorie di beni e servizi e i lavori che le aziende e gli enti del SSR acquisiscono informa aggregata o comunque facendo ricorso alle attività del Soggetto aggregatoredi cui all’articolo 20 ,comma 3. 2. Gli enti e le agenzie regionali predispongono annualmente un piano delle acquisizioni di lavori, beni e servizi, che trasmettono alla Regione entro il 31 di‐cembre di ogni anno ai fini della programmazione di cui al comma 3. 3. Fermo restando quanto previsto dal comma 3 dell’articolo 9 del d.l. 66/2014, convertito, con modificazioni, dalla l. 89/2014, con deliberazione di Giunta regio‐nale da adottarsi entro trenta giorni dalla data di entrata in vigore del bilancio diprevisione, sulla base dell’analisi svolta dalla struttura regionale competente e inragione delle risorse messe a disposizione dal bilancio regionale, sono individuatile categorie di beni e servizi e i lavori che la Regione e i soggetti di cui al comma 2acquisiscono in forma aggregata o comunque facendo ricorso alle attività del Sog‐getto aggregatore di cui all’articolo 20, comma 3, nell’anno di riferimento. 4. Nelle more dell’adozione degli atti di programmazione di cui ai commi pre‐ cedenti, il Soggetto aggregatore continua ad assicurare in favore della Regione,degli enti e delle aziende del SSR lo svolgimento delle attività di cui al comma 3dell’articolo 20 già avviate alla data di entrata in vigore della presente legge. 5. Nelle more dell’approvazione dei piani di cui ai commi 1 e 2, a decorrere dalla data di entrata in vigore della presente legge, le aziende ed enti del SSR, non‐ché gli enti e agenzie regionali possono continuare a provvedere autonomamenteall’acquisizione di lavori, beni e servizi. Gli atti indittivi delle procedure per l’acqui‐sizione di beni e servizi e i conseguenti contratti stipulati con gli operatori econo‐mici risultati aggiudicatari devono prevedere espressamente la facoltà delle aziende ed enti del SSR, nonché degli enti e agenzie regionali, di recedere in qual‐siasi tempo dal contratto, previa formale comunicazione all’appaltatore con pre‐avviso non inferiore a quindici giorni e fermo restando il pagamento delleprestazioni già eseguite, nel caso in cui, all’esito di apposita istruttoria tecnica, ri‐sulti conveniente far luogo all’acquisizione della parte residua della fornitura o delservizio aderendo alle convenzioni o agli accordi quadro stipulati dal Soggetto ag‐gregatore. Non si fa luogo al recesso ove l’appaltatore acconsenta alla rinegozia‐zione del contratto al fine di allinearlo con le condizioni previste dalle convenzionio dagli accordi quadro stipulati dal Soggetto aggregatore. 6. Il mancato rispetto delle disposizioni di cui al comma 5 costituisce motivo di valutazione negativa dell’operato degli organi di nomina regionale all’interno delleaziende, enti e agenzie di cui ai commi 1 e 2 ai fini della sussistenza della giustacausa di revoca dell’incarico da parte della Regione, fermi restando gli ulteriorieventuali profili di responsabilità. 7. Per il funzionamento del Soggetto aggregatore e per la realizzazione degli interventi di razionalizzazione della spesa mediante aggregazione degli acquisti dilavori, beni e servizi a livello regionale di cui alla presente legge, è istituito nel bi‐lancio regionale autonomo, nell’ambito della U.P .B. 08.03.01, il capitolo di spesan. 3415, denominato “Spese per la costituzione e il funzionamento del Soggettoaggregatore regionale per l’acquisizione di lavori, beni e servizi”, con una dotazionefinanziaria per l’esercizio finanziario 2014, in termini di competenza e cassa, di euro1 milione. E’ altresì istituito nel bilancio regionale autonomo, nell’ambito dellaU.P .B. 03.03.01, il capitolo di entrata n. 3310000, denominato “Soggetto aggrega‐tore regionale per l’acquisizione di lavori, beni e servizi. Entrate da convenzione”. Art. 22 Modalità di organizzazione amministrativa  per la aggregazione della spesa della Regione 1. Con atto di alta organizzazione, in applicazione dell’articolo 42, comma 2, lettera h), dello Statuto della Regione Puglia e dell’articolo 59 della legge regionaleBollettino Ufficiale della Regione Puglia ‐ n. 109 dell’8‐08‐2014 ‐ volume primo 27904\\n\\n\\n\\n## Page 7\\n\\n\\n### Text on Page 7\\n\\n7 gennaio 2004, n. 1 ( Disposizioni per la formazione del bilancio di previsione 2004 e bilancio pluriennale 2004‐2006 della Regione Puglia ), la Regione individua la struttura regionale competente di cui all’articolo 21, l’organizzazione e le relativefunzioni prevedendo che tale struttura: a) svolga le attività istruttorie e di analisi propedeutiche all’adozione delle deliberazioni di Giunta regionale di cui al comma 6 dell’articolo 20 e aicommi 1, 2 e 3 dell’articolo 21, svolgendo attività di raccordo tra le strut‐ture regionali e collaborando con la direzione dell’ Area Politiche per lapromozione della salute e con gli enti e le agenzie regionali; b) sovrintenda e, se del caso, specifichi con determinazioni dirigenziali le modalità di esecuzione di quanto disposto con le deliberazioni regionalidi cui alla lettera a) da parte delle strutture amministrative regionali. Atal fine, la struttura coordina le diverse strutture regionali acquisendo gliatti di impegno propedeutici alle procedure di acquisizione; c) svolga le funzioni di stazione appaltante della Regione nei casi in cui tale funzione non sia affidata al Soggetto aggregatore; d) monitori i prezzi di aggiudicazione e verifichi l’effettivo rispetto da parte delle strutture regionali e dei soggetti di cui al comma 5, lettera a), del‐l’articolo 20 di quanto disposto negli atti di programmazione. 2. L’atto di cui al comma 1 deve prevedere altresì che nello svolgimento dei compiti e delle funzioni di cui al comma 1 la struttura amministrativa regionale in‐dividuata operi in raccordo con il Soggetto aggregatore regionale. Art. 23 Norme per il funzionamento dei Consorzi di bonifica 1. Al fine di consentire l’attuazione della legge regionale 21 giugno 2011, n. 12 (Norme straordinarie per i consorzi di bonifica ) e della legge regionale 13 marzo 2012, n. 4 ( Nuove norme in materia di bonifica integrale e di riordino dei consorzi di bonifica ), la Regione Puglia provvede a erogare ai consorzi di bonifica commis‐ sariati, fino alla concorrenza di 8 milioni e 500 mila euro per il secondo semestre2014, le somme occorrenti per fare fronte alle seguenti spese di funzionamento:a) emolumenti ai dipendenti a tempo indeterminato e a tempo determi‐ nato; b) consumi di acqua ed energia per uso civile e agricolo;c) oneri, a carico dei consorzi, spettanti ai dipendenti collocati in quiescenza fino al 31 dicembre 2014. 2. Agli oneri di cui al comma 1 si provvede mediante l’utilizzo delle risorse fi‐ nanziarie iscritte, in termini di competenza e cassa, al capitolo n. 112091 ‐ U.P .B.01.04.04, del bilancio di previsione per l’esercizio finanziario 2014, come integratecon il presente articolo. 3. Agli adempimenti di cui al comma 1 provvedono il commissario ad acta e la struttura di supporto nominati ai sensi del comma 3 dell’articolo 42 della l.r.45/2013, con le attribuzioni ivi richiamate. I connessi oneri trovano copertura nel‐l’ambito dello stanziamento annuale ivi previsto. 4. Al comma 1 dell’articolo 29 della l.r. 4/2012, le parole “cinque” e “tre” sono sostituite dalle parole “nove” e “sette”. 5. Le lettere n) e o) del comma 4 dell’articolo 35 della l.r. 4/2012 sono sop‐ presse. 6. A partire dall’esercizio finanziario 2015 i consorzi di bonifica redigono i bi‐ lanci di previsione e consuntivi per centri di costo definendo le spese dirette e in‐dirette necessarie a erogare i servizi in modo da conseguire il pareggio tra i costireali sostenuti e i canoni applicati.  Art. 24 Contributi di bonifica 1. Per l’anno 2014, i consorzi di bonifica commissariati sono autorizzati a so‐ spendere la riscossione del tributo 630 relativo agli immobili urbani ricadenti nellearee comunali delimitate ai sensi dell’articolo 4 del decreto legislativo 30 aprile1992, n. 285 ( Nuovo codice della strada ), per la verifica della sostenibilità delle eventuali anomalie.Bollettino Ufficiale della Regione Puglia ‐ n. 109 dell’8‐08‐2014 ‐ volume primo 27905\\n\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_docs_md_pypdf2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9bf041",
   "metadata": {},
   "outputs": [
    {
     "ename": "APIError",
     "evalue": "litellm.APIError: APIError: Lm_studioException - Connection error.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/httpx/_transports/default.py:101\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py:101\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py:78\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m     ssl_object = stream.get_extra_info(\u001b[33m\"\u001b[39m\u001b[33mssl_object\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py:124\u001b[39m, in \u001b[36mHTTPConnection._connect\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mconnect_tcp\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_backend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_tcp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m     trace.return_value = stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py:207\u001b[39m, in \u001b[36mSyncBackend.connect_tcp\u001b[39m\u001b[34m(self, host, port, timeout, local_address, socket_options)\u001b[39m\n\u001b[32m    202\u001b[39m exc_map: ExceptionMapping = {\n\u001b[32m    203\u001b[39m     socket.timeout: ConnectTimeout,\n\u001b[32m    204\u001b[39m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[32m    205\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28mself\u001b[39m.gen.throw(typ, value, traceback)\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/httpcore/_exceptions.py:14\u001b[39m, in \u001b[36mmap_exceptions\u001b[39m\u001b[34m(map)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: [Errno 61] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/openai/_base_client.py:996\u001b[39m, in \u001b[36mSyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m    995\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m996\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    997\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/httpx/_transports/default.py:249\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_httpcore_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28mself\u001b[39m.gen.throw(typ, value, traceback)\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/httpx/_transports/default.py:118\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    117\u001b[39m message = \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: [Errno 61] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mAPIConnectionError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py:727\u001b[39m, in \u001b[36mOpenAIChatCompletion.completion\u001b[39m\u001b[34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[39m\n\u001b[32m    726\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m727\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    728\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m OpenAIError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py:654\u001b[39m, in \u001b[36mOpenAIChatCompletion.completion\u001b[39m\u001b[34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[39m\n\u001b[32m    642\u001b[39m logging_obj.pre_call(\n\u001b[32m    643\u001b[39m     \u001b[38;5;28minput\u001b[39m=messages,\n\u001b[32m    644\u001b[39m     api_key=openai_client.api_key,\n\u001b[32m   (...)\u001b[39m\u001b[32m    650\u001b[39m     },\n\u001b[32m    651\u001b[39m )\n\u001b[32m    653\u001b[39m headers, response = (\n\u001b[32m--> \u001b[39m\u001b[32m654\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmake_sync_openai_chat_completion_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mopenai_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopenai_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    660\u001b[39m )\n\u001b[32m    662\u001b[39m logging_obj.model_call_details[\u001b[33m\"\u001b[39m\u001b[33mresponse_headers\u001b[39m\u001b[33m\"\u001b[39m] = headers\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_utils.py:145\u001b[39m, in \u001b[36mtrack_llm_api_timing.<locals>.decorator.<locals>.sync_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py:473\u001b[39m, in \u001b[36mOpenAIChatCompletion.make_sync_openai_chat_completion_request\u001b[39m\u001b[34m(self, openai_client, data, timeout, logging_obj)\u001b[39m\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py:455\u001b[39m, in \u001b[36mOpenAIChatCompletion.make_sync_openai_chat_completion_request\u001b[39m\u001b[34m(self, openai_client, data, timeout, logging_obj)\u001b[39m\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     raw_response = \u001b[43mopenai_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    459\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/openai/_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py:279\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/openai/resources/chat/completions.py:863\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    862\u001b[39m validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m863\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    901\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    903\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    904\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    906\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/openai/_base_client.py:1283\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1280\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1281\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1282\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1283\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/openai/_base_client.py:960\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[39m\n\u001b[32m    958\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/openai/_base_client.py:1020\u001b[39m, in \u001b[36mSyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m   1019\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1020\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1029\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mRaising connection error\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/openai/_base_client.py:1098\u001b[39m, in \u001b[36mSyncAPIClient._retry_request\u001b[39m\u001b[34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[39m\n\u001b[32m   1096\u001b[39m time.sleep(timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1098\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1099\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1104\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/openai/_base_client.py:1020\u001b[39m, in \u001b[36mSyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m   1019\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1020\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1029\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mRaising connection error\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/openai/_base_client.py:1098\u001b[39m, in \u001b[36mSyncAPIClient._retry_request\u001b[39m\u001b[34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[39m\n\u001b[32m   1096\u001b[39m time.sleep(timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1098\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1099\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1104\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/openai/_base_client.py:1030\u001b[39m, in \u001b[36mSyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m   1029\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRaising connection error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1030\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request=request) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   1032\u001b[39m log.debug(\n\u001b[32m   1033\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mHTTP Response: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m,\n\u001b[32m   1034\u001b[39m     request.method,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1038\u001b[39m     response.headers,\n\u001b[32m   1039\u001b[39m )\n",
      "\u001b[31mAPIConnectionError\u001b[39m: Connection error.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/main.py:1732\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m   1726\u001b[39m     logging.post_call(\n\u001b[32m   1727\u001b[39m         \u001b[38;5;28minput\u001b[39m=messages,\n\u001b[32m   1728\u001b[39m         api_key=api_key,\n\u001b[32m   1729\u001b[39m         original_response=\u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m   1730\u001b[39m         additional_args={\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: headers},\n\u001b[32m   1731\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1732\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m optional_params.get(\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1735\u001b[39m     \u001b[38;5;66;03m## LOGGING\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/main.py:1705\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m   1704\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1705\u001b[39m     response = \u001b[43mopenai_chat_completions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1706\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1707\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1708\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1709\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1710\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1711\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1712\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1713\u001b[39m \u001b[43m        \u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m=\u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1714\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1715\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1716\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1717\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1718\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m   1719\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1720\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pass AsyncOpenAI, OpenAI client\u001b[39;49;00m\n\u001b[32m   1721\u001b[39m \u001b[43m        \u001b[49m\u001b[43morganization\u001b[49m\u001b[43m=\u001b[49m\u001b[43morganization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1722\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1723\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1724\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1725\u001b[39m     \u001b[38;5;66;03m## LOGGING - log the original exception returned\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py:738\u001b[39m, in \u001b[36mOpenAIChatCompletion.completion\u001b[39m\u001b[34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[39m\n\u001b[32m    737\u001b[39m     error_headers = \u001b[38;5;28mgetattr\u001b[39m(error_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m738\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    739\u001b[39m     status_code=status_code,\n\u001b[32m    740\u001b[39m     message=error_text,\n\u001b[32m    741\u001b[39m     headers=error_headers,\n\u001b[32m    742\u001b[39m     body=error_body,\n\u001b[32m    743\u001b[39m )\n",
      "\u001b[31mOpenAIError\u001b[39m: Connection error.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAPIError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      3\u001b[39m os.environ[\u001b[33m'\u001b[39m\u001b[33mLM_STUDIO_API_BASE\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mhttp://localhost:1234/v1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m response = \u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlm_studio/local-model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ms the weather like in Boston today in Fahrenheit?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/utils.py:1213\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1209\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[32m   1210\u001b[39m     logging_obj.failure_handler(\n\u001b[32m   1211\u001b[39m         e, traceback_exception, start_time, end_time\n\u001b[32m   1212\u001b[39m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1213\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/utils.py:1091\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1089\u001b[39m         print_verbose(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1090\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m result = \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1092\u001b[39m end_time = datetime.datetime.now()\n\u001b[32m   1093\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/main.py:3093\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m   3090\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m   3091\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   3092\u001b[39m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3093\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3094\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3095\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3096\u001b[39m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3097\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3098\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3099\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2214\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2212\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception_mapping_worked:\n\u001b[32m   2213\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mlitellm_response_headers\u001b[39m\u001b[33m\"\u001b[39m, litellm_response_headers)\n\u001b[32m-> \u001b[39m\u001b[32m2214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   2215\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2216\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m error_type \u001b[38;5;129;01min\u001b[39;00m litellm.LITELLM_EXCEPTION_TYPES:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:455\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m    453\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    454\u001b[39m         exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m APIError(\n\u001b[32m    456\u001b[39m             status_code=original_exception.status_code,\n\u001b[32m    457\u001b[39m             message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAPIError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    458\u001b[39m             llm_provider=custom_llm_provider,\n\u001b[32m    459\u001b[39m             model=model,\n\u001b[32m    460\u001b[39m             request=\u001b[38;5;28mgetattr\u001b[39m(original_exception, \u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    461\u001b[39m             litellm_debug_info=extra_information,\n\u001b[32m    462\u001b[39m         )\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    464\u001b[39m     \u001b[38;5;66;03m# if no status code then it is an APIConnectionError: https://github.com/openai/openai-python#handling-errors\u001b[39;00m\n\u001b[32m    465\u001b[39m     \u001b[38;5;66;03m# exception_mapping_worked = True\u001b[39;00m\n\u001b[32m    466\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(\n\u001b[32m    467\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAPIConnectionError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    468\u001b[39m         llm_provider=custom_llm_provider,\n\u001b[32m   (...)\u001b[39m\u001b[32m    473\u001b[39m         ),\n\u001b[32m    474\u001b[39m     )\n",
      "\u001b[31mAPIError\u001b[39m: litellm.APIError: APIError: Lm_studioException - Connection error."
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "import os\n",
    "os.environ['LM_STUDIO_API_BASE'] = \"http://localhost:1234/v1\"\n",
    "\n",
    "response = completion(\n",
    "    model=\"lm_studio/local-model\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What's the weather like in Boston today in Fahrenheit?\",\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5aa23dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LITELLM_MODEL=\"local-model\"\n",
    "!export LITELLM_API_BASE=\"http://localhost:1234/v1\"\n",
    "!export LITELLM_API_KEY=\"sk-no-key-needed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb69e37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/16 17:41:03 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entities={'Commissioni permanenti', 'leggi della Regione Puglia', 'Presidente della Regione', 'Organi giurisdizionali', 'Corte Costituzionale', 'Consiglio Regionale', 'Giunta regionale', 'Assemblea', 'Presidente del Consiglio Regionale'} edges={'riguardanti'} relations={('Corte Costituzionale', 'riguardanti', 'leggi della Regione Puglia')} entity_clusters=None edge_clusters=None\n"
     ]
    }
   ],
   "source": [
    "from kg_gen import KGGen\n",
    "import litellm\n",
    "\n",
    "# ✅ Use just the alias string here\n",
    "kg = KGGen(\n",
    "    model=\"ollama_chat/gemma3:4b\",                # string alias, not a dict\n",
    "    temperature=0.0,\n",
    "    api_key=\"sk-no-key-needed\",\n",
    ")\n",
    "\n",
    "# Now test it\n",
    "text_input = \"Il Bollettino Ufficiale della Regione Puglia si pubblica con frequenza infrasettimanale ed è diviso in due parti. Nella parte I sono pubblicati: a) sentenze ed ordinanze della Corte Costituzionale riguardanti leggi della Regione Puglia; b) ricorsi e sentenze di Organi giurisdizionali che prevedono un coinvolgimento della Regione Puglia;c) leggi e regolamenti regionali;d) deliberazioni del Consiglio Regionale riguardanti la convalida degli eletti;e) atti e circolari aventi rilevanza esterna;f) comunicati ufficiali emanati dal Presidente della Regione e dal Presidente del Consiglio Regionale;g) atti relativi all’elezione dell’Ufficio di Presidenza dell’ Assemblea, della Giunta regionale, delle Commissioni permanenti e loro eventuali dimissioni; h) deliberazioni, atti e provved\"\n",
    "graph = kg.generate(input_data=text_input, context=\"Family relationships\")\n",
    "print(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe28bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9ec7d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_docs_md_pypdf2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Generate individual graphs for each markdown text\u001b[39;00m\n\u001b[32m     11\u001b[39m graph_list = []\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text_input \u001b[38;5;129;01min\u001b[39;00m \u001b[43mlist_docs_md_pypdf2\u001b[49m:\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mProcessing text input:\u001b[39m\u001b[33m\"\u001b[39m, text_input[:\u001b[32m1000\u001b[39m])  \u001b[38;5;66;03m# Print first 100 characters for context\u001b[39;00m\n\u001b[32m     14\u001b[39m     graph = kg.generate(input_data=text_input, context=\u001b[33m\"\u001b[39m\u001b[33mBusiness regulations\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'list_docs_md_pypdf2' is not defined"
     ]
    }
   ],
   "source": [
    "from kg_gen import KGGen\n",
    "import json\n",
    "\n",
    "# Initialize KGGen as before\n",
    "kg = KGGen(\n",
    "    model=\"ollama_chat/gemma3:4b\",\n",
    "    temperature=0.0,\n",
    "    api_key=\"sk-no-key-needed\",\n",
    ")\n",
    "\n",
    "# Generate individual graphs for each markdown text\n",
    "graph_list = []\n",
    "for text_input in list_docs_md_pypdf2:\n",
    "    print(\"Processing text input:\", text_input[:1000])  # Print first 100 characters for context\n",
    "    graph = kg.generate(input_data=text_input, context=\"Business regulations\")\n",
    "    print(\"Individual Graph:\", graph)\n",
    "    graph_list.append(graph)\n",
    "\n",
    "# Aggregate the individual graphs into one final graph\n",
    "aggregated_graph = kg.aggregate(graph_list)\n",
    "print(\"Aggregated Graph:\", aggregated_graph)\n",
    "# Assuming aggregated_graph is your graph object (a dict)\n",
    "with open(\"aggregated_graph.json\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "    json.dump(aggregated_graph, outfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Graph saved to aggregated_graph.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51635b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Assuming aggregated_graph is your graph object (a dict)\n",
    "with open(\"aggregated_graph.json\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "    json.dump(aggregated_graph, outfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Graph saved to aggregated_graph.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e49aed6",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "litellm.BadRequestError: GetLLMProvider Exception - 'dict' object has no attribute 'split'\n\noriginal model: {'model_name': 'local-model-name', 'litellm_params': {'api_base': 'http://localhost:1234/v1', 'api_key': 'sk-no-key-needed', 'model': 'local-model', 'custom_llm_provider': 'openai'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py:118\u001b[39m, in \u001b[36mget_llm_provider\u001b[39m\u001b[34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m# check if llm provider provided\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m# AZURE AI-Studio Logic - Azure AI Studio supports AZURE/Cohere\u001b[39;00m\n\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# If User passes azure/command-r-plus -> we should send it to cohere_chat/command-r-plus\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mazure\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_non_openai_azure_model(model):\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'split'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/utils.py:1091\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1090\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m result = \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1092\u001b[39m end_time = datetime.datetime.now()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/main.py:3093\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m   3091\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   3092\u001b[39m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3093\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_type(\n\u001b[32m   3094\u001b[39m         model=model,\n\u001b[32m   3095\u001b[39m         custom_llm_provider=custom_llm_provider,\n\u001b[32m   3096\u001b[39m         original_exception=e,\n\u001b[32m   3097\u001b[39m         completion_kwargs=args,\n\u001b[32m   3098\u001b[39m         extra_kwargs=kwargs,\n\u001b[32m   3099\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/main.py:984\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m    983\u001b[39m     custom_llm_provider = \u001b[33m\"\u001b[39m\u001b[33mazure\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m984\u001b[39m model, custom_llm_provider, dynamic_api_key, api_base = \u001b[43mget_llm_provider\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    989\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    991\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    992\u001b[39m     provider_specific_header \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    993\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m provider_specific_header[\u001b[33m\"\u001b[39m\u001b[33mcustom_llm_provider\u001b[39m\u001b[33m\"\u001b[39m] == custom_llm_provider\n\u001b[32m    994\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py:361\u001b[39m, in \u001b[36mget_llm_provider\u001b[39m\u001b[34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[39m\n\u001b[32m    358\u001b[39m error_str = (\n\u001b[32m    359\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGetLLMProvider Exception - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33moriginal model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    360\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m litellm.exceptions.BadRequestError(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    362\u001b[39m     message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGetLLMProvider Exception - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33moriginal model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    363\u001b[39m     model=model,\n\u001b[32m    364\u001b[39m     response=httpx.Response(\n\u001b[32m    365\u001b[39m         status_code=\u001b[32m400\u001b[39m,\n\u001b[32m    366\u001b[39m         content=error_str,\n\u001b[32m    367\u001b[39m         request=httpx.Request(method=\u001b[33m\"\u001b[39m\u001b[33mcompletion\u001b[39m\u001b[33m\"\u001b[39m, url=\u001b[33m\"\u001b[39m\u001b[33mhttps://github.com/BerriAI/litellm\u001b[39m\u001b[33m\"\u001b[39m),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    368\u001b[39m     ),\n\u001b[32m    369\u001b[39m     llm_provider=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    370\u001b[39m )\n",
      "\u001b[31mBadRequestError\u001b[39m: litellm.BadRequestError: GetLLMProvider Exception - 'dict' object has no attribute 'split'\n\noriginal model: {'model_name': 'local-model-name', 'litellm_params': {'api_base': 'http://localhost:1234/v1', 'api_key': 'sk-no-key-needed', 'model': 'local-model', 'custom_llm_provider': 'openai'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/dspy/adapters/chat_adapter.py:42\u001b[39m, in \u001b[36mChatAdapter.__call__\u001b[39m\u001b[34m(self, lm, lm_kwargs, signature, demos, inputs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdemos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     44\u001b[39m     \u001b[38;5;66;03m# fallback to JSONAdapter\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/dspy/adapters/base.py:51\u001b[39m, in \u001b[36mAdapter.__call__\u001b[39m\u001b[34m(self, lm, lm_kwargs, signature, demos, inputs)\u001b[39m\n\u001b[32m     49\u001b[39m inputs = \u001b[38;5;28mself\u001b[39m.format(signature, demos, inputs)\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m outputs = \u001b[43mlm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_post_process(outputs, signature)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/dspy/utils/callback.py:326\u001b[39m, in \u001b[36mwith_callbacks.<locals>.sync_wrapper\u001b[39m\u001b[34m(instance, *args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callbacks:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m call_id = uuid.uuid4().hex\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/dspy/clients/base_lm.py:88\u001b[39m, in \u001b[36mBaseLM.__call__\u001b[39m\u001b[34m(self, prompt, messages, **kwargs)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;129m@with_callbacks\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompt=\u001b[38;5;28;01mNone\u001b[39;00m, messages=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m._process_lm_response(response, prompt, messages, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/dspy/clients/lm.py:127\u001b[39m, in \u001b[36mLM.forward\u001b[39m\u001b[34m(self, prompt, messages, **kwargs)\u001b[39m\n\u001b[32m    125\u001b[39m completion, litellm_cache_args = \u001b[38;5;28mself\u001b[39m._get_cached_completion_fn(completion, cache, enable_memory_cache)\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m results = \u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlitellm_cache_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(c.finish_reason == \u001b[33m\"\u001b[39m\u001b[33mlength\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m results[\u001b[33m\"\u001b[39m\u001b[33mchoices\u001b[39m\u001b[33m\"\u001b[39m]):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/dspy/clients/cache.py:232\u001b[39m, in \u001b[36mrequest_cache.<locals>.decorator.<locals>.sync_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    231\u001b[39m \u001b[38;5;66;03m# Otherwise, compute and store the result\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[38;5;66;03m# `enable_memory_cache` can be provided at call time to avoid indefinite growth.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/dspy/clients/lm.py:304\u001b[39m, in \u001b[36mlitellm_completion\u001b[39m\u001b[34m(request, num_retries, cache)\u001b[39m\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream_completion \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlitellm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexponential_backoff_retry\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m stream_completion()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/utils.py:1193\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1192\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_retries\u001b[39m\u001b[33m\"\u001b[39m] = num_retries\n\u001b[32m-> \u001b[39m\u001b[32m1193\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlitellm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion_with_retries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m   1195\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(e, litellm.exceptions.ContextWindowExceededError)\n\u001b[32m   1196\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m context_window_fallback_dict\n\u001b[32m   1197\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m context_window_fallback_dict\n\u001b[32m   1198\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_litellm_router_call\n\u001b[32m   1199\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/main.py:3131\u001b[39m, in \u001b[36mcompletion_with_retries\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   3128\u001b[39m     retryer = tenacity.Retrying(\n\u001b[32m   3129\u001b[39m         stop=tenacity.stop_after_attempt(num_retries), reraise=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3130\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m3131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretryer\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/tenacity/__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/tenacity/__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/tenacity/__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/tenacity/__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m     \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/tenacity/__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/utils.py:1213\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1210\u001b[39m     logging_obj.failure_handler(\n\u001b[32m   1211\u001b[39m         e, traceback_exception, start_time, end_time\n\u001b[32m   1212\u001b[39m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1213\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/utils.py:1091\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1090\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m result = \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1092\u001b[39m end_time = datetime.datetime.now()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/main.py:3093\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m   3091\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   3092\u001b[39m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3093\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_type(\n\u001b[32m   3094\u001b[39m         model=model,\n\u001b[32m   3095\u001b[39m         custom_llm_provider=custom_llm_provider,\n\u001b[32m   3096\u001b[39m         original_exception=e,\n\u001b[32m   3097\u001b[39m         completion_kwargs=args,\n\u001b[32m   3098\u001b[39m         extra_kwargs=kwargs,\n\u001b[32m   3099\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/main.py:984\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m    983\u001b[39m     custom_llm_provider = \u001b[33m\"\u001b[39m\u001b[33mazure\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m984\u001b[39m model, custom_llm_provider, dynamic_api_key, api_base = \u001b[43mget_llm_provider\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    989\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    991\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    992\u001b[39m     provider_specific_header \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    993\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m provider_specific_header[\u001b[33m\"\u001b[39m\u001b[33mcustom_llm_provider\u001b[39m\u001b[33m\"\u001b[39m] == custom_llm_provider\n\u001b[32m    994\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py:361\u001b[39m, in \u001b[36mget_llm_provider\u001b[39m\u001b[34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[39m\n\u001b[32m    358\u001b[39m error_str = (\n\u001b[32m    359\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGetLLMProvider Exception - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33moriginal model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    360\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m litellm.exceptions.BadRequestError(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    362\u001b[39m     message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGetLLMProvider Exception - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33moriginal model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    363\u001b[39m     model=model,\n\u001b[32m    364\u001b[39m     response=httpx.Response(\n\u001b[32m    365\u001b[39m         status_code=\u001b[32m400\u001b[39m,\n\u001b[32m    366\u001b[39m         content=error_str,\n\u001b[32m    367\u001b[39m         request=httpx.Request(method=\u001b[33m\"\u001b[39m\u001b[33mcompletion\u001b[39m\u001b[33m\"\u001b[39m, url=\u001b[33m\"\u001b[39m\u001b[33mhttps://github.com/BerriAI/litellm\u001b[39m\u001b[33m\"\u001b[39m),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    368\u001b[39m     ),\n\u001b[32m    369\u001b[39m     llm_provider=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    370\u001b[39m )\n",
      "\u001b[31mBadRequestError\u001b[39m: litellm.BadRequestError: GetLLMProvider Exception - 'dict' object has no attribute 'split'\n\noriginal model: {'model_name': 'local-model-name', 'litellm_params': {'api_base': 'http://localhost:1234/v1', 'api_key': 'sk-no-key-needed', 'model': 'local-model', 'custom_llm_provider': 'openai'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py:118\u001b[39m, in \u001b[36mget_llm_provider\u001b[39m\u001b[34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m# check if llm provider provided\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m# AZURE AI-Studio Logic - Azure AI Studio supports AZURE/Cohere\u001b[39;00m\n\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# If User passes azure/command-r-plus -> we should send it to cohere_chat/command-r-plus\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mazure\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_non_openai_azure_model(model):\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'split'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/utils.py:1091\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1090\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m result = \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1092\u001b[39m end_time = datetime.datetime.now()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/main.py:3093\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m   3091\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   3092\u001b[39m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3093\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_type(\n\u001b[32m   3094\u001b[39m         model=model,\n\u001b[32m   3095\u001b[39m         custom_llm_provider=custom_llm_provider,\n\u001b[32m   3096\u001b[39m         original_exception=e,\n\u001b[32m   3097\u001b[39m         completion_kwargs=args,\n\u001b[32m   3098\u001b[39m         extra_kwargs=kwargs,\n\u001b[32m   3099\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/main.py:984\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m    983\u001b[39m     custom_llm_provider = \u001b[33m\"\u001b[39m\u001b[33mazure\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m984\u001b[39m model, custom_llm_provider, dynamic_api_key, api_base = \u001b[43mget_llm_provider\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    989\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    991\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    992\u001b[39m     provider_specific_header \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    993\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m provider_specific_header[\u001b[33m\"\u001b[39m\u001b[33mcustom_llm_provider\u001b[39m\u001b[33m\"\u001b[39m] == custom_llm_provider\n\u001b[32m    994\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py:361\u001b[39m, in \u001b[36mget_llm_provider\u001b[39m\u001b[34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[39m\n\u001b[32m    358\u001b[39m error_str = (\n\u001b[32m    359\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGetLLMProvider Exception - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33moriginal model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    360\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m litellm.exceptions.BadRequestError(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    362\u001b[39m     message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGetLLMProvider Exception - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33moriginal model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    363\u001b[39m     model=model,\n\u001b[32m    364\u001b[39m     response=httpx.Response(\n\u001b[32m    365\u001b[39m         status_code=\u001b[32m400\u001b[39m,\n\u001b[32m    366\u001b[39m         content=error_str,\n\u001b[32m    367\u001b[39m         request=httpx.Request(method=\u001b[33m\"\u001b[39m\u001b[33mcompletion\u001b[39m\u001b[33m\"\u001b[39m, url=\u001b[33m\"\u001b[39m\u001b[33mhttps://github.com/BerriAI/litellm\u001b[39m\u001b[33m\"\u001b[39m),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    368\u001b[39m     ),\n\u001b[32m    369\u001b[39m     llm_provider=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    370\u001b[39m )\n",
      "\u001b[31mBadRequestError\u001b[39m: litellm.BadRequestError: GetLLMProvider Exception - 'dict' object has no attribute 'split'\n\noriginal model: {'model_name': 'local-model-name', 'litellm_params': {'api_base': 'http://localhost:1234/v1', 'api_key': 'sk-no-key-needed', 'model': 'local-model', 'custom_llm_provider': 'openai'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# EXAMPLE 1: Single string with context\u001b[39;00m\n\u001b[32m     21\u001b[39m text_input = \u001b[33m\"\u001b[39m\u001b[33mLinda is Josh\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms mother. Ben is Josh\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms brother. Andrew is Josh\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms father.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m graph_1 = \u001b[43mkg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m  \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m  \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFamily relationships\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     25\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Output: \u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# entities={'Linda', 'Ben', 'Andrew', 'Josh'} \u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# edges={'is brother of', 'is father of', 'is mother of'} \u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# relations={('Ben', 'is brother of', 'Josh'), \u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m#           ('Andrew', 'is father of', 'Josh'), \u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m#           ('Linda', 'is mother of', 'Josh')}\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/kg_gen/kg_gen.py:133\u001b[39m, in \u001b[36mKGGen.generate\u001b[39m\u001b[34m(self, input_data, model, api_key, api_base, context, chunk_size, cluster, temperature, output_folder)\u001b[39m\n\u001b[32m    125\u001b[39m   \u001b[38;5;28mself\u001b[39m.init_model(\n\u001b[32m    126\u001b[39m     model=model \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    127\u001b[39m     temperature=temperature \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.temperature,\n\u001b[32m    128\u001b[39m     api_key=api_key \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.api_key,\n\u001b[32m    129\u001b[39m     api_base=api_base \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.api_base,\n\u001b[32m    130\u001b[39m   )\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunk_size:\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m   entities = \u001b[43mget_entities\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdspy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessed_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_conversation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_conversation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m   relations = get_relations(\u001b[38;5;28mself\u001b[39m.dspy, processed_input, entities, is_conversation=is_conversation)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/kg_gen/steps/_1_get_entities.py:25\u001b[39m, in \u001b[36mget_entities\u001b[39m\u001b[34m(dspy, input_data, is_conversation)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     23\u001b[39m   extract = dspy.Predict(TextEntities)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m result = \u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result.entities\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/dspy/predict/predict.py:85\u001b[39m, in \u001b[36mPredict.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args:\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;28mself\u001b[39m._get_positional_args_error_message())\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/dspy/utils/callback.py:326\u001b[39m, in \u001b[36mwith_callbacks.<locals>.sync_wrapper\u001b[39m\u001b[34m(instance, *args, **kwargs)\u001b[39m\n\u001b[32m    324\u001b[39m callbacks = _get_active_callbacks(instance)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callbacks:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m call_id = uuid.uuid4().hex\n\u001b[32m    330\u001b[39m _execute_start_callbacks(instance, fn, call_id, callbacks, args, kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/dspy/primitives/program.py:32\u001b[39m, in \u001b[36mModule.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     29\u001b[39m         output.set_lm_usage(usage_tracker.get_total_tokens())\n\u001b[32m     30\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/dspy/predict/predict.py:146\u001b[39m, in \u001b[36mPredict.forward\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m settings.context(send_stream=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m         completions = \u001b[43madapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m=\u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdemos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdemos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_postprocess(completions, signature, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/dspy/adapters/chat_adapter.py:51\u001b[39m, in \u001b[36mChatAdapter.__call__\u001b[39m\u001b[34m(self, lm, lm_kwargs, signature, demos, inputs)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ContextWindowExceededError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, JSONAdapter):\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# On context window exceeded error or already using JSONAdapter, we don't want to retry with a different\u001b[39;00m\n\u001b[32m     49\u001b[39m     \u001b[38;5;66;03m# adapter.\u001b[39;00m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mJSONAdapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdemos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/dspy/adapters/json_adapter.py:53\u001b[39m, in \u001b[36mJSONAdapter.__call__\u001b[39m\u001b[34m(self, lm, lm_kwargs, signature, demos, inputs)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# If response_format is not supported, use basic call\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m params \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdemos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Check early for open-ended mapping types before trying structured outputs.\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _has_open_ended_mapping(signature):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/dspy/adapters/chat_adapter.py:50\u001b[39m, in \u001b[36mChatAdapter.__call__\u001b[39m\u001b[34m(self, lm, lm_kwargs, signature, demos, inputs)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdspy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01madapters\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjson_adapter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m JSONAdapter\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ContextWindowExceededError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, JSONAdapter):\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# On context window exceeded error or already using JSONAdapter, we don't want to retry with a different\u001b[39;00m\n\u001b[32m     49\u001b[39m     \u001b[38;5;66;03m# adapter.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m JSONAdapter()(lm, lm_kwargs, signature, demos, inputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/dspy/adapters/chat_adapter.py:42\u001b[39m, in \u001b[36mChatAdapter.__call__\u001b[39m\u001b[34m(self, lm, lm_kwargs, signature, demos, inputs)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m     34\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     35\u001b[39m     lm: LM,\n\u001b[32m   (...)\u001b[39m\u001b[32m     39\u001b[39m     inputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m     40\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdemos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     44\u001b[39m         \u001b[38;5;66;03m# fallback to JSONAdapter\u001b[39;00m\n\u001b[32m     45\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdspy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01madapters\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjson_adapter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m JSONAdapter\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/dspy/adapters/base.py:51\u001b[39m, in \u001b[36mAdapter.__call__\u001b[39m\u001b[34m(self, lm, lm_kwargs, signature, demos, inputs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m     42\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     43\u001b[39m     lm: \u001b[33m\"\u001b[39m\u001b[33mLM\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     47\u001b[39m     inputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m     48\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m     49\u001b[39m     inputs = \u001b[38;5;28mself\u001b[39m.format(signature, demos, inputs)\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     outputs = \u001b[43mlm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_post_process(outputs, signature)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/dspy/utils/callback.py:326\u001b[39m, in \u001b[36mwith_callbacks.<locals>.sync_wrapper\u001b[39m\u001b[34m(instance, *args, **kwargs)\u001b[39m\n\u001b[32m    324\u001b[39m callbacks = _get_active_callbacks(instance)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callbacks:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m call_id = uuid.uuid4().hex\n\u001b[32m    330\u001b[39m _execute_start_callbacks(instance, fn, call_id, callbacks, args, kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/dspy/clients/base_lm.py:88\u001b[39m, in \u001b[36mBaseLM.__call__\u001b[39m\u001b[34m(self, prompt, messages, **kwargs)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;129m@with_callbacks\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompt=\u001b[38;5;28;01mNone\u001b[39;00m, messages=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m._process_lm_response(response, prompt, messages, **kwargs)\n\u001b[32m     91\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/dspy/clients/lm.py:127\u001b[39m, in \u001b[36mLM.forward\u001b[39m\u001b[34m(self, prompt, messages, **kwargs)\u001b[39m\n\u001b[32m    124\u001b[39m completion = litellm_completion \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_type == \u001b[33m\"\u001b[39m\u001b[33mchat\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m litellm_text_completion\n\u001b[32m    125\u001b[39m completion, litellm_cache_args = \u001b[38;5;28mself\u001b[39m._get_cached_completion_fn(completion, cache, enable_memory_cache)\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m results = \u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlitellm_cache_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(c.finish_reason == \u001b[33m\"\u001b[39m\u001b[33mlength\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m results[\u001b[33m\"\u001b[39m\u001b[33mchoices\u001b[39m\u001b[33m\"\u001b[39m]):\n\u001b[32m    134\u001b[39m     logger.warning(\n\u001b[32m    135\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLM response was truncated due to exceeding max_tokens=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.kwargs[\u001b[33m'\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    136\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou can inspect the latest LM interactions with `dspy.inspect_history()`. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    139\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m if the reason for truncation is repetition.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    140\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/dspy/clients/cache.py:232\u001b[39m, in \u001b[36mrequest_cache.<locals>.decorator.<locals>.sync_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cached_result\n\u001b[32m    231\u001b[39m \u001b[38;5;66;03m# Otherwise, compute and store the result\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[38;5;66;03m# `enable_memory_cache` can be provided at call time to avoid indefinite growth.\u001b[39;00m\n\u001b[32m    234\u001b[39m cache.put(modified_request, result, ignored_args_for_cache_key, enable_memory_cache)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/dspy/clients/lm.py:304\u001b[39m, in \u001b[36mlitellm_completion\u001b[39m\u001b[34m(request, num_retries, cache)\u001b[39m\n\u001b[32m    302\u001b[39m stream_completion = _get_stream_completion_fn(request, cache, sync=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream_completion \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlitellm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexponential_backoff_retry\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m stream_completion()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/utils.py:1193\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1187\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1188\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(e, openai.APIError)\n\u001b[32m   1189\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, openai.Timeout)\n\u001b[32m   1190\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, openai.APIConnectionError)\n\u001b[32m   1191\u001b[39m     ):\n\u001b[32m   1192\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_retries\u001b[39m\u001b[33m\"\u001b[39m] = num_retries\n\u001b[32m-> \u001b[39m\u001b[32m1193\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlitellm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion_with_retries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m   1195\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(e, litellm.exceptions.ContextWindowExceededError)\n\u001b[32m   1196\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m context_window_fallback_dict\n\u001b[32m   1197\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m context_window_fallback_dict\n\u001b[32m   1198\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_litellm_router_call\n\u001b[32m   1199\u001b[39m ):\n\u001b[32m   1200\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/main.py:3131\u001b[39m, in \u001b[36mcompletion_with_retries\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   3127\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3128\u001b[39m     retryer = tenacity.Retrying(\n\u001b[32m   3129\u001b[39m         stop=tenacity.stop_after_attempt(num_retries), reraise=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3130\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m3131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretryer\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/tenacity/__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/tenacity/__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/tenacity/__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    418\u001b[39m retry_exc = \u001b[38;5;28mself\u001b[39m.retry_error_cls(fut)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/tenacity/__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> t.NoReturn:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/tenacity/__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/utils.py:1213\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1209\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[32m   1210\u001b[39m     logging_obj.failure_handler(\n\u001b[32m   1211\u001b[39m         e, traceback_exception, start_time, end_time\n\u001b[32m   1212\u001b[39m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1213\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/utils.py:1091\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1089\u001b[39m         print_verbose(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1090\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m result = \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1092\u001b[39m end_time = datetime.datetime.now()\n\u001b[32m   1093\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/main.py:3093\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m   3090\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m   3091\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   3092\u001b[39m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3093\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_type(\n\u001b[32m   3094\u001b[39m         model=model,\n\u001b[32m   3095\u001b[39m         custom_llm_provider=custom_llm_provider,\n\u001b[32m   3096\u001b[39m         original_exception=e,\n\u001b[32m   3097\u001b[39m         completion_kwargs=args,\n\u001b[32m   3098\u001b[39m         extra_kwargs=kwargs,\n\u001b[32m   3099\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/main.py:984\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m    982\u001b[39m     model = deployment_id\n\u001b[32m    983\u001b[39m     custom_llm_provider = \u001b[33m\"\u001b[39m\u001b[33mazure\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m984\u001b[39m model, custom_llm_provider, dynamic_api_key, api_base = \u001b[43mget_llm_provider\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    989\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    991\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    992\u001b[39m     provider_specific_header \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    993\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m provider_specific_header[\u001b[33m\"\u001b[39m\u001b[33mcustom_llm_provider\u001b[39m\u001b[33m\"\u001b[39m] == custom_llm_provider\n\u001b[32m    994\u001b[39m ):\n\u001b[32m    995\u001b[39m     headers.update(provider_specific_header[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py:361\u001b[39m, in \u001b[36mget_llm_provider\u001b[39m\u001b[34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     error_str = (\n\u001b[32m    359\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGetLLMProvider Exception - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33moriginal model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    360\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m litellm.exceptions.BadRequestError(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    362\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGetLLMProvider Exception - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33moriginal model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    363\u001b[39m         model=model,\n\u001b[32m    364\u001b[39m         response=httpx.Response(\n\u001b[32m    365\u001b[39m             status_code=\u001b[32m400\u001b[39m,\n\u001b[32m    366\u001b[39m             content=error_str,\n\u001b[32m    367\u001b[39m             request=httpx.Request(method=\u001b[33m\"\u001b[39m\u001b[33mcompletion\u001b[39m\u001b[33m\"\u001b[39m, url=\u001b[33m\"\u001b[39m\u001b[33mhttps://github.com/BerriAI/litellm\u001b[39m\u001b[33m\"\u001b[39m),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    368\u001b[39m         ),\n\u001b[32m    369\u001b[39m         llm_provider=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    370\u001b[39m     )\n",
      "\u001b[31mBadRequestError\u001b[39m: litellm.BadRequestError: GetLLMProvider Exception - 'dict' object has no attribute 'split'\n\noriginal model: {'model_name': 'local-model-name', 'litellm_params': {'api_base': 'http://localhost:1234/v1', 'api_key': 'sk-no-key-needed', 'model': 'local-model', 'custom_llm_provider': 'openai'}}"
     ]
    }
   ],
   "source": [
    "from kg_gen import KGGen\n",
    "import litellm\n",
    "litellm.model_alias_map[\"local-model\"] = {\n",
    "    \"model_name\": \"local-model-name\",  # name doesn't matter, but must be consistent\n",
    "    \"litellm_params\": {\n",
    "        \"api_base\": \"http://localhost:1234/v1\",  # LM Studio server\n",
    "        \"api_key\": \"sk-no-key-needed\",          # Dummy key\n",
    "        \"model\": \"local-model\",            # This will be passed to LM Studio\n",
    "        \"custom_llm_provider\": \"openai\"         # <- This is the crucial fix\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize KGGen with optional configuration\n",
    "kg = KGGen(\n",
    "  model=\"local-model\",  # Default model\n",
    "  temperature=0.0,        # Default temperature\n",
    "  api_key=\"YOUR_API_KEY\"  # Optional if set in environment or using a local model\n",
    ")\n",
    "\n",
    "# EXAMPLE 1: Single string with context\n",
    "text_input = \"Linda is Josh's mother. Ben is Josh's brother. Andrew is Josh's father.\"\n",
    "graph_1 = kg.generate(\n",
    "  input_data=text_input,\n",
    "  context=\"Family relationships\"\n",
    ")\n",
    "# Output: \n",
    "# entities={'Linda', 'Ben', 'Andrew', 'Josh'} \n",
    "# edges={'is brother of', 'is father of', 'is mother of'} \n",
    "# relations={('Ben', 'is brother of', 'Josh'), \n",
    "#           ('Andrew', 'is father of', 'Josh'), \n",
    "#           ('Linda', 'is mother of', 'Josh')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77a1bc4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=local-model\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlitellm\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m res = \u001b[43mlitellm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlocal-model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m  \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHi!\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(res[\u001b[33m'\u001b[39m\u001b[33mchoices\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/utils.py:1213\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1209\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[32m   1210\u001b[39m     logging_obj.failure_handler(\n\u001b[32m   1211\u001b[39m         e, traceback_exception, start_time, end_time\n\u001b[32m   1212\u001b[39m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1213\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/utils.py:1091\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1089\u001b[39m         print_verbose(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1090\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m result = \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1092\u001b[39m end_time = datetime.datetime.now()\n\u001b[32m   1093\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/main.py:3093\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m   3090\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m   3091\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   3092\u001b[39m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3093\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_type(\n\u001b[32m   3094\u001b[39m         model=model,\n\u001b[32m   3095\u001b[39m         custom_llm_provider=custom_llm_provider,\n\u001b[32m   3096\u001b[39m         original_exception=e,\n\u001b[32m   3097\u001b[39m         completion_kwargs=args,\n\u001b[32m   3098\u001b[39m         extra_kwargs=kwargs,\n\u001b[32m   3099\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/main.py:984\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m    982\u001b[39m     model = deployment_id\n\u001b[32m    983\u001b[39m     custom_llm_provider = \u001b[33m\"\u001b[39m\u001b[33mazure\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m984\u001b[39m model, custom_llm_provider, dynamic_api_key, api_base = \u001b[43mget_llm_provider\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    989\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    991\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    992\u001b[39m     provider_specific_header \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    993\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m provider_specific_header[\u001b[33m\"\u001b[39m\u001b[33mcustom_llm_provider\u001b[39m\u001b[33m\"\u001b[39m] == custom_llm_provider\n\u001b[32m    994\u001b[39m ):\n\u001b[32m    995\u001b[39m     headers.update(provider_specific_header[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py:356\u001b[39m, in \u001b[36mget_llm_provider\u001b[39m\u001b[34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    355\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, litellm.exceptions.BadRequestError):\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m         error_str = (\n\u001b[32m    359\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGetLLMProvider Exception - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33moriginal model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    360\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/kg+llm_task2_nlp/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py:333\u001b[39m, in \u001b[36mget_llm_provider\u001b[39m\u001b[34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[39m\n\u001b[32m    331\u001b[39m     error_str = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m Pass model as E.g. For \u001b[39m\u001b[33m'\u001b[39m\u001b[33mHuggingface\u001b[39m\u001b[33m'\u001b[39m\u001b[33m inference endpoints pass in `completion(model=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mhuggingface/starcoder\u001b[39m\u001b[33m'\u001b[39m\u001b[33m,..)` Learn more: https://docs.litellm.ai/docs/providers\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    332\u001b[39m     \u001b[38;5;66;03m# maps to openai.NotFoundError, this is raised when openai does not recognize the llm\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m litellm.exceptions.BadRequestError(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    334\u001b[39m         message=error_str,\n\u001b[32m    335\u001b[39m         model=model,\n\u001b[32m    336\u001b[39m         response=httpx.Response(\n\u001b[32m    337\u001b[39m             status_code=\u001b[32m400\u001b[39m,\n\u001b[32m    338\u001b[39m             content=error_str,\n\u001b[32m    339\u001b[39m             request=httpx.Request(method=\u001b[33m\"\u001b[39m\u001b[33mcompletion\u001b[39m\u001b[33m\"\u001b[39m, url=\u001b[33m\"\u001b[39m\u001b[33mhttps://github.com/BerriAI/litellm\u001b[39m\u001b[33m\"\u001b[39m),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    340\u001b[39m         ),\n\u001b[32m    341\u001b[39m         llm_provider=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    342\u001b[39m     )\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_base \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(api_base, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    344\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[32m    345\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mapi base needs to be a string. api_base=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(api_base)\n\u001b[32m    346\u001b[39m     )\n",
      "\u001b[31mBadRequestError\u001b[39m: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=local-model\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers"
     ]
    }
   ],
   "source": [
    "import litellm\n",
    "\n",
    "res = litellm.completion(\n",
    "  model=\"local-model\",\n",
    "  messages=[{\"role\": \"user\", \"content\": \"Hi!\"}]\n",
    ")\n",
    "print(res['choices'][0]['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd7711218aeecc65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T15:10:31.863985Z",
     "start_time": "2025-06-03T15:09:37.409780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing LLM connection to LM Studio at http://localhost:1234/v1...\n",
      "LLM via LM Studio initialized successfully.\n",
      "\n",
      "Make sure your desired model is loaded and the server is RUNNING in LM Studio.\n",
      "\n",
      "Testing with Gemma prompt format (ensure Gemma is loaded in LM Studio)...\n",
      "\n",
      "--- Testing Gemma Prompt Format ---\n",
      "Sending raw prompt to Gemma (via LM Studio):\n",
      "<start_of_turn>user\n",
      "Cosa sono i Knowledge Graph?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "Response from Gemma:\n",
      "<think>\n",
      "Okay, the user is asking about \"Knowledge Graph.\" I should explain what it is in simple terms.\n",
      "\n",
      "I know that a Knowledge Graph is like a huge collection of information, kind of like Google's version of a semantic web.\n",
      "\n",
      "It organizes data into entities and relationships, making it easier to understand and use for various tasks.\n",
      "\n",
      "I should mention the types of entities, such as people, places, things, and concepts, each with their own properties.\n",
      "\n",
      "Also, the relationships between these entities are important to show how they connect.\n",
      "\n",
      "Maybe I can give an example to make it clearer, like how a person might be related to a place through their work or studies.\n",
      "\n",
      "I should keep it conversational and avoid any technical jargon that might confuse the user.\n",
      "</think>\n",
      "\n",
      "The Knowledge Graph is a semantic database developed by Google. It represents information in a structured format, similar to a graph, where entities (such as people, places, things, and concepts) are connected through relationships or attributes.\n",
      "\n",
      "For example:\n",
      "- A person may be related to a place (e.g., birthplace), an occupation, or other entities.\n",
      "- A place may be associated with other places (e.g., neighboring cities) or events.\n",
      "\n",
      "This structure allows for more efficient retrieval of information compared\n",
      "\n",
      "--- Testing Gemma Prompt Format ---\n",
      "Sending raw prompt to Gemma (via LM Studio):\n",
      "<start_of_turn>user\n",
      "Explain the concept of a Large Language Model in simple terms.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "Response from Gemma:\n",
      "<think>\n",
      "Okay, so I need to explain what a Large Language Model (LLM) is in simple terms. Hmm, where do I start? Well, I know that LLMs are related to AI and language processing. Let me think about the basic concepts first.\n",
      "\n",
      "I remember that machines can process text, but how exactly? Maybe it's something to do with training the model on a lot of data. So, an LLM is probably a type of machine learning model that's been trained on a massive amount of text from books, articles, and maybe even the internet.\n",
      "\n",
      "Wait, what does \"large\" mean here? I guess it refers to the scale of the model. Maybe it's because these models use huge amounts of data or have many parameters, which makes them large in size. But how exactly do they work?\n",
      "\n",
      "I think LLMs are good at understanding patterns in language. So, when given a prompt, they can generate text that sounds human-like. For example, if I ask them to write a story, they might come up with something coherent and interesting.\n",
      "\n",
      "But how are they different from other types of models? Oh right, there's something called a Transformer architecture which is commonly used in LLMs. I'm not entirely sure what\n",
      "\n",
      "--- Prompting Test Complete ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI # Use ChatOpenAI for LM Studio's OpenAI-compatible API\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "# --- Configuration ---\n",
    "LM_STUDIO_BASE_URL = \"http://localhost:1234/v1\" # Default LM Studio API endpoint\n",
    "LM_STUDIO_MODEL_NAME = \"local-model\" # Placeholder, model is selected in LM Studio UI\n",
    "\n",
    "# --- Initialize LLM via LM Studio API ---\n",
    "print(f\"Initializing LLM connection to LM Studio at {LM_STUDIO_BASE_URL}...\")\n",
    "try:\n",
    "    llm = ChatOpenAI(\n",
    "        openai_api_base=LM_STUDIO_BASE_URL,\n",
    "        openai_api_key=\"not-needed\", # Can be any string, LM Studio doesn't use it\n",
    "        model_name=LM_STUDIO_MODEL_NAME,\n",
    "        temperature=0.7,\n",
    "        max_tokens=256 # Adjust as needed for your tests\n",
    "    )\n",
    "    print(\"LLM via LM Studio initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing ChatOpenAI for LM Studio: {e}\")\n",
    "    print(\"Ensure the LM Studio server is running, a model is loaded, and the server is started.\")\n",
    "    exit()\n",
    "\n",
    "# --- Define and Test Prompt Formats ---\n",
    "\n",
    "def test_gemma_prompt(llm_instance, user_question):\n",
    "    \"\"\"\n",
    "    Tests the Gemma-specific instruction prompt format.\n",
    "    <start_of_turn>user\n",
    "    QUESTION<end_of_turn>\n",
    "    <start_of_turn>model\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Testing Gemma Prompt Format ---\")\n",
    "    # LangChain's ChatPromptTemplate is more structured for chat models\n",
    "    # For raw string prompts with specific model tokens, sometimes just sending the string directly is easier\n",
    "    # but ChatPromptTemplate helps maintain roles.\n",
    "\n",
    "    # Constructing the raw prompt string as Gemma expects for simple queries\n",
    "    raw_gemma_prompt = f\"<start_of_turn>user\\n{user_question}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    print(f\"Sending raw prompt to Gemma (via LM Studio):\\n{raw_gemma_prompt}\")\n",
    "\n",
    "    try:\n",
    "        # For ChatOpenAI, we typically send a list of messages\n",
    "        # We can simulate the raw prompt by putting it all in the user message,\n",
    "        # or construct a more idiomatic ChatPromptTemplate.\n",
    "        # Let's try a more structured approach first.\n",
    "        # Note: The actual <start_of_turn>model token is for the model to generate *after*.\n",
    "        #       We provide the user turn.\n",
    "\n",
    "        chat_template = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"human\", \"<start_of_turn>user\\n{question}<end_of_turn>\\n<start_of_turn>model\"),\n",
    "            ]\n",
    "        )\n",
    "        messages = chat_template.format_messages(question=user_question)\n",
    "\n",
    "        response = llm_instance.invoke(messages)\n",
    "        print(\"Response from Gemma:\")\n",
    "        if hasattr(response, 'content'):\n",
    "            print(response.content)\n",
    "        else:\n",
    "            print(response) # Fallback\n",
    "    except Exception as e:\n",
    "        print(f\"Error invoking Gemma model: {e}\")\n",
    "\n",
    "def test_llama_alpaca_prompt(llm_instance, user_question):\n",
    "    \"\"\"\n",
    "    Tests an Alpaca-style prompt format, often used for Llama-based models.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Testing Llama/Alpaca Prompt Format ---\")\n",
    "    system_message_alpaca = \"You are a helpful AI assistant.\" # Optional system message\n",
    "    alpaca_template_str = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "    chat_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            SystemMessage(content=system_message_alpaca), # Optional\n",
    "            HumanMessagePromptTemplate.from_template(alpaca_template_str),\n",
    "        ]\n",
    "    )\n",
    "    messages = chat_template.format_messages(instruction=user_question) # The user_question becomes the instruction\n",
    "\n",
    "    print(f\"Sending messages to Llama/Alpaca model (via LM Studio):\\n{messages}\")\n",
    "    try:\n",
    "        response = llm_instance.invoke(messages)\n",
    "        print(\"Response from Llama/Alpaca model:\")\n",
    "        if hasattr(response, 'content'):\n",
    "            print(response.content)\n",
    "        else:\n",
    "            print(response)\n",
    "    except Exception as e:\n",
    "        print(f\"Error invoking Llama/Alpaca model: {e}\")\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # **IMPORTANT:**\n",
    "    # 1. Start LM Studio.\n",
    "    # 2. In LM Studio, load EITHER your Gemma model OR your DeepSeek Llama model.\n",
    "    # 3. Start the LM Studio local server.\n",
    "    # 4. Then run this script.\n",
    "\n",
    "    print(\"\\nMake sure your desired model is loaded and the server is RUNNING in LM Studio.\")\n",
    "\n",
    "    # Example question\n",
    "    question = \"Cosa sono i Knowledge Graph?\"\n",
    "\n",
    "    # === UNCOMMENT THE SECTION FOR THE MODEL YOU ARE TESTING IN LM STUDIO ===\n",
    "\n",
    "    # --- Test Gemma ---\n",
    "    # (Load your Gemma 12B 4bit MLX model in LM Studio and start the server)\n",
    "    print(\"\\nTesting with Gemma prompt format (ensure Gemma is loaded in LM Studio)...\")\n",
    "    test_gemma_prompt(llm, question)\n",
    "    test_gemma_prompt(llm, \"Explain the concept of a Large Language Model in simple terms.\")\n",
    "\n",
    "\n",
    "    # --- Test DeepSeek Llama ---\n",
    "    # (Stop the server in LM Studio, load your DeepSeek R1 Distill Llama 8B, and restart the server)\n",
    "    # print(\"\\nTesting with Llama/Alpaca prompt format (ensure DeepSeek Llama is loaded in LM Studio)...\")\n",
    "    # test_llama_alpaca_prompt(llm, question)\n",
    "    # test_llama_alpaca_prompt(llm, \"Write a short story about a robot discovering music.\")\n",
    "\n",
    "    print(\"\\n--- Prompting Test Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47ce08b753c5144",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_llama_alpaca = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Use the following pieces of context to answer the question. If you don't know the answer, just say that you don't know.\n",
    "\n",
    "### Input (Context):\n",
    "{context}\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template_llama_alpaca, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a31ed82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Using cached langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.59 in ./.venv/lib/python3.11/site-packages (from langchain-community) (0.3.63)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.25 in ./.venv/lib/python3.11/site-packages (from langchain-community) (0.3.25)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.venv/lib/python3.11/site-packages (from langchain-community) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.11/site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.11/site-packages (from langchain-community) (6.0.2)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community)\n",
      "  Downloading aiohttp-3.12.7-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in ./.venv/lib/python3.11/site-packages (from langchain-community) (9.1.2)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in ./.venv/lib/python3.11/site-packages (from langchain-community) (0.3.44)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in ./.venv/lib/python3.11/site-packages (from langchain-community) (2.2.6)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading frozenlist-1.6.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading multidict-6.4.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading propcache-0.3.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading yarl-1.20.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (72 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in ./.venv/lib/python3.11/site-packages (from langchain<1.0.0,>=0.3.25->langchain-community) (0.3.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./.venv/lib/python3.11/site-packages (from langchain<1.0.0,>=0.3.25->langchain-community) (2.11.5)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.venv/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (4.14.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.venv/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.venv/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./.venv/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.11/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain-community) (2025.4.26)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.59->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (2.33.2)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.11/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
      "Downloading langchain_community-0.3.24-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.12.7-cp311-cp311-macosx_11_0_arm64.whl (467 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.6.0-cp311-cp311-macosx_11_0_arm64.whl (122 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading multidict-6.4.4-cp311-cp311-macosx_11_0_arm64.whl (37 kB)\n",
      "Downloading propcache-0.3.1-cp311-cp311-macosx_11_0_arm64.whl (45 kB)\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading yarl-1.20.0-cp311-cp311-macosx_11_0_arm64.whl (94 kB)\n",
      "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Installing collected packages: python-dotenv, propcache, mypy-extensions, multidict, marshmallow, httpx-sse, frozenlist, aiohappyeyeballs, yarl, typing-inspect, aiosignal, pydantic-settings, dataclasses-json, aiohttp, langchain-community\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.7 aiosignal-1.3.2 dataclasses-json-0.6.7 frozenlist-1.6.0 httpx-sse-0.4.0 langchain-community-0.3.24 marshmallow-3.26.1 multidict-6.4.4 mypy-extensions-1.1.0 propcache-0.3.1 pydantic-settings-2.9.1 python-dotenv-1.1.0 typing-inspect-0.9.0 yarl-1.20.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-community \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2909b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing LLM connection to LM Studio at http://localhost:1234/v1...\n",
      "LLM via LM Studio initialized successfully.\n",
      "Connecting to Neo4j at bolt://localhost:7687...\n",
      "Neo4j connection successful.\n",
      "Creating RetrievalQA chain with Neo4j retriever...\n",
      "RetrievalQA chain with Neo4j created.\n",
      "\n",
      "Make sure your Neo4j instance and LM Studio server (with Gemma/Llama loaded) are RUNNING.\n",
      "\n",
      "--- Testing RAG with Custom Neo4j Retriever ---\n",
      "\n",
      "Query: Cosa puoi dirmi a proprosito di 'DecretoLegge'?\n",
      "Neo4j Retriever: Received query: Cosa puoi dirmi a proprosito di 'DecretoLegge'?\n",
      "Neo4j Retriever: Extracted entities: ['DecretoLegge']\n",
      "Neo4j Retriever: Executing Cypher: \n",
      "        MATCH (n) WHERE toLower(n.name) CONTAINS toLower('DecretoLegge')\n",
      "        OPTIONAL MATCH (n)-[r]-(m)\n",
      "        RETURN n, r, m\n",
      "        LIMIT 5\n",
      "        \n",
      "Neo4j Retriever: Cypher results for 'DecretoLegge': 0 items\n",
      "Neo4j Retriever: Formatted context (first 300 chars):\n",
      "No specific information found for 'DecretoLegge' in the graph....\n",
      "\n",
      "Answer:\n",
      "Cannot answer based on the provided graph data. The knowledge graph does not contain any information about 'DecretoLegge'.\n",
      "\n",
      "Source Context (from Neo4j):\n",
      "- No specific information found for 'DecretoLegge' in the graph....\n",
      "\n",
      "Query: A cosa è connesso il codice penale nel grafo?\n",
      "Neo4j Retriever: Received query: A cosa è connesso il codice penale nel grafo?\n",
      "Neo4j Retriever: Extracted entities: ['codice penale', 'grafo']\n",
      "Neo4j Retriever: Executing Cypher: \n",
      "        MATCH (n) WHERE toLower(n.name) CONTAINS toLower('codice penale')\n",
      "        OPTIONAL MATCH (n)-[r]-(m)\n",
      "        RETURN n, r, m\n",
      "        LIMIT 5\n",
      "        \n",
      "Neo4j Retriever: Cypher results for 'codice penale': 0 items\n",
      "Neo4j Retriever: Executing Cypher: \n",
      "        MATCH (n) WHERE toLower(n.name) CONTAINS toLower('grafo')\n",
      "        OPTIONAL MATCH (n)-[r]-(m)\n",
      "        RETURN n, r, m\n",
      "        LIMIT 5\n",
      "        \n",
      "Neo4j Retriever: Cypher results for 'grafo': 0 items\n",
      "Neo4j Retriever: Formatted context (first 300 chars):\n",
      "No specific information found for 'codice penale' in the graph.\n",
      "No specific information found for 'grafo' in the graph....\n",
      "\n",
      "Answer:\n",
      "Cannot answer based on the provided graph data. The knowledge graph does not contain any information about 'codice penale'.\n",
      "\n",
      "Source Context (from Neo4j):\n",
      "- No specific information found for 'codice penale' in the graph.\n",
      "No specific information found for 'grafo' in the graph....\n",
      "\n",
      "Query: Descrivi le relazioni dell'articolo 7-bis.\n",
      "Neo4j Retriever: Received query: Descrivi le relazioni dell'articolo 7-bis.\n",
      "Neo4j Retriever: Extracted entities: ['Article 7-bis']\n",
      "Neo4j Retriever: Executing Cypher: \n",
      "        MATCH (n) WHERE toLower(n.name) CONTAINS toLower('Article 7-bis')\n",
      "        OPTIONAL MATCH (n)-[r]-(m)\n",
      "        RETURN n, r, m\n",
      "        LIMIT 5\n",
      "        \n",
      "Neo4j Retriever: Cypher results for 'Article 7-bis': 0 items\n",
      "Neo4j Retriever: Formatted context (first 300 chars):\n",
      "No specific information found for 'Article 7-bis' in the graph....\n",
      "\n",
      "Answer:\n",
      "Cannot answer based on the provided graph data. The knowledge graph does not contain any information about \"Article 7-bis.\"\n",
      "\n",
      "Source Context (from Neo4j):\n",
      "- No specific information found for 'Article 7-bis' in the graph....\n",
      "\n",
      "--- Neo4j RAG Test Complete ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.graphs import Neo4jGraph # For connecting to Neo4j\n",
    "from langchain.chains.graph_qa.cypher import GraphCypherQAChain # A more direct QA chain\n",
    "from langchain.schema import Document # To format graph results as Documents for some chains\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings # If doing hybrid or vector search\n",
    "\n",
    "# --- Configuration ---\n",
    "LM_STUDIO_BASE_URL = \"http://localhost:1234/v1\"\n",
    "LM_STUDIO_MODEL_NAME = \"local-model\" # Placeholder\n",
    "\n",
    "NEO4J_URI = \"bolt://localhost:7687\"  # Replace with your Neo4j URI\n",
    "NEO4J_USERNAME = \"neo4j\"             # Replace with your Neo4j username\n",
    "NEO4J_PASSWORD = \"giacomo3234\"     # Replace with your Neo4j password\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\" # For query embedding if needed\n",
    "\n",
    "# --- Initialize LLM via LM Studio API ---\n",
    "print(f\"Initializing LLM connection to LM Studio at {LM_STUDIO_BASE_URL}...\")\n",
    "try:\n",
    "    llm = ChatOpenAI(\n",
    "        openai_api_base=LM_STUDIO_BASE_URL,\n",
    "        openai_api_key=\"not-needed\",\n",
    "        model_name=LM_STUDIO_MODEL_NAME,\n",
    "        temperature=0.7,\n",
    "        max_tokens=512\n",
    "    )\n",
    "    print(\"LLM via LM Studio initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing ChatOpenAI for LM Studio: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Initialize Neo4j Connection ---\n",
    "print(f\"Connecting to Neo4j at {NEO4J_URI}...\")\n",
    "try:\n",
    "    graph = Neo4jGraph(\n",
    "        url=NEO4J_URI,\n",
    "        username=NEO4J_USERNAME,\n",
    "        password=NEO4J_PASSWORD\n",
    "    )\n",
    "    # Test connection by fetching schema (optional)\n",
    "    # print(\"Neo4j Schema:\", graph.schema)\n",
    "    print(\"Neo4j connection successful.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Neo4j: {e}\")\n",
    "    print(\"Ensure Neo4j is running and credentials are correct.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- Define Prompt Template for LLM (using Graph Context) ---\n",
    "# Adjust this based on how you format your graph context\n",
    "# and the model loaded in LM Studio (Gemma, Llama, etc.)\n",
    "prompt_template_graph_rag_gemma = \"\"\"\n",
    "<start_of_turn>user\n",
    "Use the following information retrieved from a knowledge graph to answer the question.\n",
    "If the information is not sufficient, say that you cannot answer based on the provided graph data.\n",
    "\n",
    "Knowledge Graph Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "Answer:\"\"\"\n",
    "\n",
    "PROMPT_GRAPH_RAG = PromptTemplate(\n",
    "    template=prompt_template_graph_rag_gemma, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "\n",
    "# --- Custom Neo4j Retriever (Illustrative Example) ---\n",
    "# This is where you define how to get relevant data from Neo4j.\n",
    "# This part is HIGHLY dependent on your graph schema and data.\n",
    "\n",
    "def neo4j_retriever_function(query: str, graph_db: Neo4jGraph, llm_for_entity_extraction=None):\n",
    "    \"\"\"\n",
    "    A simplified retriever.\n",
    "    1. (Optional) Use LLM to extract entities from the query.\n",
    "    2. Construct a Cypher query based on these entities.\n",
    "    3. Execute and format results.\n",
    "    \"\"\"\n",
    "    print(f\"Neo4j Retriever: Received query: {query}\")\n",
    "    extracted_entities = []\n",
    "\n",
    "    # Step 1: (Optional) Entity Extraction from query using LLM\n",
    "    if llm_for_entity_extraction:\n",
    "        try:\n",
    "            entity_prompt = PromptTemplate.from_template(\n",
    "                \"<start_of_turn>user\\nExtract key entities (people, organizations, concepts, product names) from this question: '{user_query}'\\nReturn as a comma-separated list.<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "            )\n",
    "            entity_chain = entity_prompt | llm_for_entity_extraction\n",
    "            response = entity_chain.invoke({\"user_query\": query})\n",
    "            extracted_entities = [e.strip() for e in response.content.split(',') if e.strip()]\n",
    "            print(f\"Neo4j Retriever: Extracted entities: {extracted_entities}\")\n",
    "        except Exception as e_entity:\n",
    "            print(f\"Neo4j Retriever: Error extracting entities: {e_entity}\")\n",
    "            # Fallback: use the whole query as a keyword if entity extraction fails\n",
    "            if not extracted_entities:\n",
    "                 extracted_entities = [query.split(\" \")[-1]] # very naive fallback\n",
    "\n",
    "    if not extracted_entities: # If no LLM or extraction failed, use some keywords from query\n",
    "        # This is a very naive way to get \"keywords\"\n",
    "        keywords = [word for word in query.lower().split() if len(word) > 3 and word not in \n",
    "                    [\"what\", \"who\", \"is\", \"are\", \"the\", \"of\"]]\n",
    "        extracted_entities = keywords[:2] # Take first 2\n",
    "        print(f\"Neo4j Retriever: Using keywords as entities: {extracted_entities}\")\n",
    "\n",
    "\n",
    "    # Step 2 & 3: Construct and Execute Cypher Query\n",
    "    # THIS IS THE PART YOU MUST CUSTOMIZE FOR YOUR GRAPH\n",
    "    # Example: Assuming you have nodes with a 'name' property and you want to find them\n",
    "    # and their direct relationships.\n",
    "    graph_context_parts = []\n",
    "    for entity_name in extracted_entities:\n",
    "        # Example Cypher: Find node by name and its 1-hop neighbors\n",
    "        # ADJUST THE LABEL (e.g., :Entity, :Product, :Person) and PROPERTY (e.g., name, title)\n",
    "        cypher_query = f\"\"\"\n",
    "        MATCH (n) WHERE toLower(n.name) CONTAINS toLower('{entity_name}')\n",
    "        OPTIONAL MATCH (n)-[r]-(m)\n",
    "        RETURN n, r, m\n",
    "        LIMIT 5\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"Neo4j Retriever: Executing Cypher: {cypher_query}\")\n",
    "            results = graph_db.query(cypher_query)\n",
    "            print(f\"Neo4j Retriever: Cypher results for '{entity_name}': {len(results)} items\")\n",
    "\n",
    "            if results:\n",
    "                for record in results:\n",
    "                    node_n_props = {k: v for k, v in record.get(\"n\", {}).items() if v} # Filter out None values\n",
    "                    node_m_props = {k: v for k, v in record.get(\"m\", {}).items() if v}\n",
    "                    rel_r_type = record.get(\"r\").type if record.get(\"r\") else None # Access type property\n",
    "\n",
    "                    part = f\"Node: {node_n_props}\"\n",
    "                    if rel_r_type and node_m_props:\n",
    "                        part += f\" -[{rel_r_type}]-> Node: {node_m_props}\"\n",
    "                    graph_context_parts.append(part)\n",
    "            else:\n",
    "                 graph_context_parts.append(f\"No specific information found for '{entity_name}' in the graph.\")\n",
    "\n",
    "        except Exception as e_cypher:\n",
    "            print(f\"Neo4j Retriever: Error executing Cypher for '{entity_name}': {e_cypher}\")\n",
    "            graph_context_parts.append(f\"Error retrieving data for '{entity_name}'.\")\n",
    "\n",
    "\n",
    "    if not graph_context_parts:\n",
    "        return [Document(page_content=\"No relevant information found in the knowledge graph.\")]\n",
    "\n",
    "    # Format results as LangChain Documents\n",
    "    # You might want a more sophisticated way to summarize or structure this text\n",
    "    formatted_context = \"\\n\".join(graph_context_parts)\n",
    "    print(f\"Neo4j Retriever: Formatted context (first 300 chars):\\n{formatted_context[:300]}...\")\n",
    "    return [Document(page_content=formatted_context)]\n",
    "\n",
    "\n",
    "# --- Create a Custom RetrievalQA Chain ---\n",
    "# LangChain's standard RetrievalQA expects a retriever object that has a `get_relevant_documents` method.\n",
    "# We can wrap our function in a simple class.\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from typing import List\n",
    "\n",
    "class Neo4jCustomRetriever(BaseRetriever):\n",
    "    graph_db: Neo4jGraph\n",
    "    llm_for_entities: ChatOpenAI # Pass the LLM for entity extraction\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
    "    ) -> List[Document]:\n",
    "        return neo4j_retriever_function(query, self.graph_db, self.llm_for_entities)\n",
    "\n",
    "custom_neo4j_retriever = Neo4jCustomRetriever(graph_db=graph, llm_for_entities=llm)\n",
    "\n",
    "print(\"Creating RetrievalQA chain with Neo4j retriever...\")\n",
    "# This chain will now use our custom_neo4j_retriever\n",
    "qa_chain_neo4j = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\", # \"stuff\" the graph context into the prompt\n",
    "    retriever=custom_neo4j_retriever,\n",
    "    return_source_documents=True, # Source documents will be the formatted graph context\n",
    "    chain_type_kwargs={\"prompt\": PROMPT_GRAPH_RAG}\n",
    ")\n",
    "print(\"RetrievalQA chain with Neo4j created.\")\n",
    "\n",
    "# --- Alternative: GraphCypherQAChain (More direct, LLM generates Cypher) ---\n",
    "# This chain lets the LLM try to generate the Cypher query directly.\n",
    "# It can be powerful but requires the LLM to be good at Cypher and understand your schema.\n",
    "# print(\"Creating GraphCypherQAChain (LLM generates Cypher)...\")\n",
    "# cypher_qa_chain = GraphCypherQAChain.from_llm(\n",
    "#     graph=graph,\n",
    "#     cypher_llm=llm, # LLM to generate Cypher\n",
    "#     qa_llm=llm,     # LLM to answer based on Cypher results\n",
    "#     verbose=True,\n",
    "#     # You might need to provide schema information in the prompt or validate queries\n",
    "#     # return_intermediate_steps=True # Good for debugging\n",
    "# )\n",
    "# print(\"GraphCypherQAChain created.\")\n",
    "\n",
    "\n",
    "# --- Ask Questions ---\n",
    "def ask_rag_question(query, chain):\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    try:\n",
    "        result = chain.invoke({\"query\": query})\n",
    "        print(\"\\nAnswer:\")\n",
    "        # Result from ChatOpenAI will be an AIMessage, access content with .content\n",
    "        response_content = result.get(\"result\", \"\")\n",
    "        if hasattr(response_content, 'content'):\n",
    "             print(response_content.content)\n",
    "        else:\n",
    "            print(response_content)\n",
    "\n",
    "        if result.get(\"source_documents\"):\n",
    "            print(\"\\nSource Context (from Neo4j):\")\n",
    "            for doc in result[\"source_documents\"]:\n",
    "                print(f\"- {doc.page_content[:200]}...\") # Show first 200 chars of graph context\n",
    "    except Exception as e:\n",
    "        print(f\"Error during RAG chain invocation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nMake sure your Neo4j instance and LM Studio server (with Gemma/Llama loaded) are RUNNING.\")\n",
    "\n",
    "    # --- Using Custom Neo4j Retriever RAG ---\n",
    "    print(\"\\n--- Testing RAG with Custom Neo4j Retriever ---\")\n",
    "    # **ADAPT THESE QUESTIONS TO YOUR ACTUAL NEO4J DATA**\n",
    "    ask_rag_question(\"Cosa puoi dirmi a proprosito di 'DecretoLegge'?\", qa_chain_neo4j)\n",
    "    ask_rag_question(\"A cosa è connesso il codice penale nel grafo?\", qa_chain_neo4j)\n",
    "    ask_rag_question(\"Descrivi le relazioni dell'articolo 7-bis.\", qa_chain_neo4j)\n",
    "\n",
    "\n",
    "    # --- Using GraphCypherQAChain (Alternative - LLM writes Cypher) ---\n",
    "    # print(\"\\n--- Testing RAG with GraphCypherQAChain ---\")\n",
    "    # **ADAPT THESE QUESTIONS. THE LLM NEEDS TO UNDERSTAND YOUR SCHEMA TO WRITE CYPHER.**\n",
    "    # For this to work well, you might need to give the LLM schema info or few-shot examples.\n",
    "    # try:\n",
    "    #     response_cypher = cypher_qa_chain.invoke({\"query\": \"Who are the directors of the movie The Matrix?\"}) # Example\n",
    "    #     print(\"Cypher QA Answer:\", response_cypher[\"result\"])\n",
    "    #     # if response_cypher.get(\"intermediate_steps\"):\n",
    "    #     #     print(\"Intermediate Cypher Steps:\", response_cypher[\"intermediate_steps\"])\n",
    "    # except Exception as e_cypher_chain:\n",
    "    #     print(f\"Error with GraphCypherQAChain: {e_cypher_chain}\")\n",
    "    #     print(\"This chain is more complex and relies on the LLM's ability to generate valid Cypher for your schema.\")\n",
    "\n",
    "    print(\"\\n--- Neo4j RAG Test Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
