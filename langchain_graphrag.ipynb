{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7739eebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade --quiet  langchain langchain-neo4j langchain-openai langchain-experimental neo4j pypdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "985afe48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration set.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# --- LangChain Imports ---\n",
    "from langchain_core.documents import Document\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_openai import ChatOpenAI # We use this class to connect to any OpenAI-compatible server\n",
    "\n",
    "# --- Neo4j Imports ---\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "LM_STUDIO_API_BASE = \"http://localhost:1234/v1\"\n",
    "LM_STUDIO_API_KEY = \"not-needed\"\n",
    "\n",
    "# --- Neo4j Database Configuration ---\n",
    "NEO4J_URI = \"neo4j://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"giacomo3234\" # <--- IMPORTANT: SET YOUR PASSWORD\n",
    "\n",
    "print(\"Configuration set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a58f23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: docs/Legge regionale n_37_2014 artt. 20-21-22.pdf\n",
      "Processing file: docs/Direttiva 2014_25_UE.pdf\n",
      "Processing file: docs/Direttiva 2014_23_UE.pdf\n",
      "Processing file: docs/Decreto Legislativo 7 marzo 2005_agg_L_147_2013.pdf\n",
      "Processing file: docs/L. 27 Dicembre 2006 n.296 (Finanziaria 2007).pdf\n",
      "Processing file: docs/L. 23 Dicembre 2000 n.388 (Finanziaria 2001).pdf\n",
      "Processing file: docs/dPR 5 ottobre 2010_207_agg_DM_infrastrutture_24apr2014.pdf\n",
      "Processing file: docs/Direttiva 2014_24_UE.pdf\n",
      "Processing file: docs/D.Lgs. 50_2016.pdf\n",
      "Processing file: docs/DGR_17_2024_01_22_signed_signed.pdf\n",
      "Processing file: docs/Decreto legislativo 12 aprile  2006_163_agg_DL_24apr2014_n_66.pdf\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "import os\n",
    "\n",
    "def clean_text_for_markdown_pypdf2(text):\n",
    "    \"\"\"Cleans text for better Markdown display, handling multiple newlines.\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)  # Consolidate paragraph breaks\n",
    "    text = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text)  # Replace single newlines with space\n",
    "    return text.strip()\n",
    "\n",
    "def pypdf2_to_markdown_chunks(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF using PyPDF2 and splits it per page.\n",
    "    \n",
    "    Returns:\n",
    "        list of str: Each string is a Markdown formatted page.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "    except Exception as e:\n",
    "        return [f\"<!-- Error opening PDF {pdf_path}: {e} -->\"]\n",
    "\n",
    "    chunks = []\n",
    "    for page_num, page in enumerate(reader.pages):\n",
    "        page_md = f\"\\n## Page {page_num + 1}\\n\\n\"\n",
    "        try:\n",
    "            # Extract and clean the page text\n",
    "            page_text = page.extract_text()\n",
    "            cleaned_text = clean_text_for_markdown_pypdf2(page_text)\n",
    "            if cleaned_text:\n",
    "                page_md += f\"### Text on Page {page_num + 1}\\n\" + cleaned_text + \"\\n\"\n",
    "            else:\n",
    "                page_md += f\"<!-- No text found on Page {page_num + 1} -->\\n\"\n",
    "        except Exception as e:\n",
    "            page_md += f\"<!-- Error extracting text from Page {page_num + 1}: {e} -->\\n\"\n",
    "        chunks.append(page_md)\n",
    "    return chunks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"docs\"  # Folder containing PDF files\n",
    "    list_docs_md = []\n",
    "    \n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"The folder '{folder_path}' does not exist. Please provide a valid folder path.\")\n",
    "    else:\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith(\".pdf\"):\n",
    "                pdf_file_path = os.path.join(folder_path, file_name)\n",
    "                print(f\"Processing file: {pdf_file_path}\")\n",
    "                # Get each page separately, as Markdown content\n",
    "                page_chunks = pypdf2_to_markdown_chunks(pdf_file_path)\n",
    "                for chunk in page_chunks:\n",
    "                    # Create a Document for each page with associated PDF name in metadata\n",
    "                    doc = Document(page_content=chunk, metadata={\"pdf_name\": file_name})\n",
    "                    list_docs_md.append(doc)\n",
    "                    \n",
    "    print(\"Total documents created:\", len(list_docs_md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d07019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain components initialized and pointing to LM Studio.\n"
     ]
    }
   ],
   "source": [
    "# --- Initialize the LangChain LLM, pointing to LM Studio ---\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    base_url=LM_STUDIO_API_BASE,\n",
    "    api_key=LM_STUDIO_API_KEY,\n",
    "    model_name=\"google/gemma3:4b\" \n",
    ")\n",
    "\n",
    "llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "\n",
    "print(\"LangChain components initialized and pointing to LM Studio.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6c08ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Page 1\n",
      "\n",
      "### Text on Page 1\n",
      "REPUBBLICA ITALIANA ANNO XLV BARI, 8 AGOSTO 2014 n. 109BOLLETTINO UFFICIALE della Regione Puglia Leggi e regolamenti regionali VOLUME PRIMO 2014.08. 19  09:09:08 +02'00'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(list_docs_md[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b3cfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting graph extraction... ---\n",
      "This may take a while depending on the document size and your computer's performance.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Starting graph extraction... ---\")\n",
    "print(\"This may take a while depending on the document size and your computer's performance.\")\n",
    "\n",
    "# The transformer converts the text documents into graph documents (nodes and relationships)\n",
    "graph_documents = await llm_transformer.aconvert_to_graph_documents(list_docs_md)\n",
    "\n",
    "# --- Inspect the results ---\n",
    "# The result is a list, but we only processed one large document.\n",
    "main_graph = graph_documents[0]\n",
    "\n",
    "print(f\"\\n--- Extraction Complete! ---\")\n",
    "print(f\"Number of nodes: {len(main_graph.nodes)}\")\n",
    "print(f\"Number of relationships: {len(main_graph.relationships)}\")\n",
    "\n",
    "print(\"\\n--- Sample Nodes ---\")\n",
    "for node in main_graph.nodes[:5]:\n",
    "    print(node)\n",
    "\n",
    "print(\"\\n--- Sample Relationships ---\")\n",
    "for rel in main_graph.relationships[:5]:\n",
    "    print(rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08310332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_graph_to_neo4j(graph_doc, uri, user, password):\n",
    "    \"\"\"Uploads a LangChain GraphDocument to Neo4j.\"\"\"\n",
    "    driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "    \n",
    "    # Use a set to avoid duplicate node creation queries\n",
    "    nodes = {node.id: node for node in graph_doc.nodes}\n",
    "    \n",
    "    with driver.session(database=\"neo4j\") as session:\n",
    "        # Clear existing data\n",
    "        print(\"Clearing existing Neo4j database...\")\n",
    "        session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "        \n",
    "        # Create uniqueness constraint for faster merging\n",
    "        session.run(\"CREATE CONSTRAINT IF NOT EXISTS FOR (e:__Entity__) REQUIRE e.id IS UNIQUE\")\n",
    "        \n",
    "        # Upload nodes\n",
    "        print(f\"Uploading {len(nodes)} nodes...\")\n",
    "        upload_nodes_query = \"\"\"\n",
    "        UNWIND $nodes AS node_data\n",
    "        MERGE (e:__Entity__ {id: node_data.id})\n",
    "        // Dynamically set the label and properties\n",
    "        // The label is capitalized from the node type\n",
    "        // The node id is also set as a 'name' property for easy viewing\n",
    "        WITH e, node_data\n",
    "        CALL apoc.create.setLabels(e, [apoc.text.upper(node_data.type)]) YIELD node\n",
    "        SET node.name = node_data.id\n",
    "        RETURN count(node)\n",
    "        \"\"\"\n",
    "        session.run(upload_nodes_query, nodes=list(nodes.values()))\n",
    "        \n",
    "        # Upload relationships\n",
    "        print(f\"Uploading {len(graph_doc.relationships)} relationships...\")\n",
    "        upload_rels_query = \"\"\"\n",
    "        UNWIND $rels AS rel_data\n",
    "        MATCH (source:__Entity__ {id: rel_data.source.id})\n",
    "        MATCH (target:__Entity__ {id: rel_data.target.id})\n",
    "        // Use the relationship type as the graph relationship type\n",
    "        CALL apoc.create.relationship(source, rel_data.type, {}, target) YIELD rel\n",
    "        RETURN count(rel)\n",
    "        \"\"\"\n",
    "        session.run(upload_rels_query, rels=[rel.dict() for rel in graph_doc.relationships])\n",
    "        \n",
    "        # Remove the generic __Entity__ label now that specific labels are set\n",
    "        session.run(\"MATCH (e:__Entity__) REMOVE e:__Entity__\")\n",
    "        \n",
    "    driver.close()\n",
    "    print(\"Upload to Neo4j complete!\")\n",
    "\n",
    "# --- Run the upload process ---\n",
    "print(\"\\n--- Starting upload to Neo4j... ---\")\n",
    "upload_graph_to_neo4j(main_graph, NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
